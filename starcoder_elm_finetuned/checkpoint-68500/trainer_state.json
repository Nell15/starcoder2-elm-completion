{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.9936194388602395,
  "eval_steps": 500,
  "global_step": 68500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.00437024735600035,
      "grad_norm": 6.129732131958008,
      "learning_rate": 0.0002995760860064679,
      "loss": 3.5875,
      "step": 100
    },
    {
      "epoch": 0.0087404947120007,
      "grad_norm": 7.08195686340332,
      "learning_rate": 0.0002991390612708679,
      "loss": 3.0504,
      "step": 200
    },
    {
      "epoch": 0.013110742068001049,
      "grad_norm": 3.915440559387207,
      "learning_rate": 0.0002987064067826239,
      "loss": 2.8379,
      "step": 300
    },
    {
      "epoch": 0.0174809894240014,
      "grad_norm": 6.8583292961120605,
      "learning_rate": 0.00029827375229437984,
      "loss": 2.6965,
      "step": 400
    },
    {
      "epoch": 0.021851236780001747,
      "grad_norm": 9.229962348937988,
      "learning_rate": 0.0002978367275587798,
      "loss": 2.6253,
      "step": 500
    },
    {
      "epoch": 0.026221484136002098,
      "grad_norm": 3.653895139694214,
      "learning_rate": 0.00029739970282317976,
      "loss": 2.5304,
      "step": 600
    },
    {
      "epoch": 0.030591731492002446,
      "grad_norm": 5.32478141784668,
      "learning_rate": 0.00029696267808757975,
      "loss": 2.3634,
      "step": 700
    },
    {
      "epoch": 0.0349619788480028,
      "grad_norm": 2.0253701210021973,
      "learning_rate": 0.00029652565335197973,
      "loss": 2.4062,
      "step": 800
    },
    {
      "epoch": 0.039332226204003146,
      "grad_norm": 4.4656982421875,
      "learning_rate": 0.00029608862861637967,
      "loss": 2.309,
      "step": 900
    },
    {
      "epoch": 0.043702473560003494,
      "grad_norm": 4.971871376037598,
      "learning_rate": 0.00029565160388077966,
      "loss": 2.308,
      "step": 1000
    },
    {
      "epoch": 0.04807272091600385,
      "grad_norm": 5.648033618927002,
      "learning_rate": 0.0002952145791451796,
      "loss": 2.2965,
      "step": 1100
    },
    {
      "epoch": 0.052442968272004196,
      "grad_norm": 3.019197940826416,
      "learning_rate": 0.0002947775544095795,
      "loss": 2.3463,
      "step": 1200
    },
    {
      "epoch": 0.056813215628004544,
      "grad_norm": 4.467976093292236,
      "learning_rate": 0.0002943405296739795,
      "loss": 2.2771,
      "step": 1300
    },
    {
      "epoch": 0.06118346298400489,
      "grad_norm": 6.8788933753967285,
      "learning_rate": 0.0002939035049383795,
      "loss": 2.2915,
      "step": 1400
    },
    {
      "epoch": 0.06555371034000525,
      "grad_norm": 3.445840358734131,
      "learning_rate": 0.00029346648020277944,
      "loss": 2.241,
      "step": 1500
    },
    {
      "epoch": 0.0699239576960056,
      "grad_norm": 1.9251694679260254,
      "learning_rate": 0.0002930294554671794,
      "loss": 2.1535,
      "step": 1600
    },
    {
      "epoch": 0.07429420505200594,
      "grad_norm": 4.906696796417236,
      "learning_rate": 0.00029259243073157936,
      "loss": 2.1612,
      "step": 1700
    },
    {
      "epoch": 0.07866445240800629,
      "grad_norm": 5.8120341300964355,
      "learning_rate": 0.00029215540599597935,
      "loss": 2.1931,
      "step": 1800
    },
    {
      "epoch": 0.08303469976400664,
      "grad_norm": 5.106973171234131,
      "learning_rate": 0.00029171838126037933,
      "loss": 2.1193,
      "step": 1900
    },
    {
      "epoch": 0.08740494712000699,
      "grad_norm": 3.155665636062622,
      "learning_rate": 0.00029128135652477927,
      "loss": 2.1862,
      "step": 2000
    },
    {
      "epoch": 0.09177519447600734,
      "grad_norm": 5.920391082763672,
      "learning_rate": 0.00029084433178917926,
      "loss": 2.0132,
      "step": 2100
    },
    {
      "epoch": 0.0961454418320077,
      "grad_norm": 4.873442649841309,
      "learning_rate": 0.0002904073070535792,
      "loss": 2.1125,
      "step": 2200
    },
    {
      "epoch": 0.10051568918800804,
      "grad_norm": 4.013099193572998,
      "learning_rate": 0.0002899702823179792,
      "loss": 2.1107,
      "step": 2300
    },
    {
      "epoch": 0.10488593654400839,
      "grad_norm": 6.833703994750977,
      "learning_rate": 0.00028953325758237917,
      "loss": 2.0378,
      "step": 2400
    },
    {
      "epoch": 0.10925618390000874,
      "grad_norm": 3.9026825428009033,
      "learning_rate": 0.0002890962328467791,
      "loss": 2.1697,
      "step": 2500
    },
    {
      "epoch": 0.11362643125600909,
      "grad_norm": 5.9186177253723145,
      "learning_rate": 0.0002886592081111791,
      "loss": 2.0453,
      "step": 2600
    },
    {
      "epoch": 0.11799667861200944,
      "grad_norm": 6.0350518226623535,
      "learning_rate": 0.000288222183375579,
      "loss": 2.0528,
      "step": 2700
    },
    {
      "epoch": 0.12236692596800978,
      "grad_norm": 2.8954758644104004,
      "learning_rate": 0.000287785158639979,
      "loss": 2.0026,
      "step": 2800
    },
    {
      "epoch": 0.12673717332401013,
      "grad_norm": 5.907834529876709,
      "learning_rate": 0.000287348133904379,
      "loss": 1.9838,
      "step": 2900
    },
    {
      "epoch": 0.1311074206800105,
      "grad_norm": 5.180283069610596,
      "learning_rate": 0.00028691110916877893,
      "loss": 1.9666,
      "step": 3000
    },
    {
      "epoch": 0.13547766803601083,
      "grad_norm": 5.9106879234313965,
      "learning_rate": 0.0002864740844331789,
      "loss": 1.8472,
      "step": 3100
    },
    {
      "epoch": 0.1398479153920112,
      "grad_norm": 5.17655611038208,
      "learning_rate": 0.00028603705969757886,
      "loss": 1.9267,
      "step": 3200
    },
    {
      "epoch": 0.14421816274801152,
      "grad_norm": 4.196730613708496,
      "learning_rate": 0.0002856000349619788,
      "loss": 1.9953,
      "step": 3300
    },
    {
      "epoch": 0.1485884101040119,
      "grad_norm": 5.695321083068848,
      "learning_rate": 0.0002851630102263788,
      "loss": 1.9991,
      "step": 3400
    },
    {
      "epoch": 0.15295865746001225,
      "grad_norm": 3.34873628616333,
      "learning_rate": 0.00028472598549077877,
      "loss": 1.995,
      "step": 3500
    },
    {
      "epoch": 0.15732890481601258,
      "grad_norm": 1.6597532033920288,
      "learning_rate": 0.0002842889607551787,
      "loss": 1.9587,
      "step": 3600
    },
    {
      "epoch": 0.16169915217201294,
      "grad_norm": 1.5773756504058838,
      "learning_rate": 0.0002838519360195787,
      "loss": 1.9818,
      "step": 3700
    },
    {
      "epoch": 0.16606939952801328,
      "grad_norm": 5.0505523681640625,
      "learning_rate": 0.0002834149112839786,
      "loss": 1.9869,
      "step": 3800
    },
    {
      "epoch": 0.17043964688401364,
      "grad_norm": 3.4902212619781494,
      "learning_rate": 0.0002829778865483786,
      "loss": 1.9633,
      "step": 3900
    },
    {
      "epoch": 0.17480989424001397,
      "grad_norm": 3.020583391189575,
      "learning_rate": 0.0002825408618127786,
      "loss": 1.9552,
      "step": 4000
    },
    {
      "epoch": 0.17918014159601434,
      "grad_norm": 3.2952516078948975,
      "learning_rate": 0.00028210383707717853,
      "loss": 1.9672,
      "step": 4100
    },
    {
      "epoch": 0.18355038895201467,
      "grad_norm": 4.617979049682617,
      "learning_rate": 0.0002816668123415785,
      "loss": 1.9644,
      "step": 4200
    },
    {
      "epoch": 0.18792063630801503,
      "grad_norm": 2.727048397064209,
      "learning_rate": 0.00028122978760597846,
      "loss": 1.947,
      "step": 4300
    },
    {
      "epoch": 0.1922908836640154,
      "grad_norm": 4.550250053405762,
      "learning_rate": 0.00028079276287037844,
      "loss": 1.8948,
      "step": 4400
    },
    {
      "epoch": 0.19666113102001573,
      "grad_norm": 3.468456506729126,
      "learning_rate": 0.00028035573813477843,
      "loss": 1.8835,
      "step": 4500
    },
    {
      "epoch": 0.2010313783760161,
      "grad_norm": 4.251574993133545,
      "learning_rate": 0.00027991871339917837,
      "loss": 1.9563,
      "step": 4600
    },
    {
      "epoch": 0.20540162573201642,
      "grad_norm": 3.654862403869629,
      "learning_rate": 0.00027948168866357836,
      "loss": 1.9384,
      "step": 4700
    },
    {
      "epoch": 0.20977187308801679,
      "grad_norm": 4.92299747467041,
      "learning_rate": 0.0002790446639279783,
      "loss": 1.9321,
      "step": 4800
    },
    {
      "epoch": 0.21414212044401712,
      "grad_norm": 3.035623788833618,
      "learning_rate": 0.0002786076391923783,
      "loss": 1.9869,
      "step": 4900
    },
    {
      "epoch": 0.21851236780001748,
      "grad_norm": 3.6926307678222656,
      "learning_rate": 0.00027817061445677827,
      "loss": 1.8463,
      "step": 5000
    },
    {
      "epoch": 0.22288261515601784,
      "grad_norm": 2.0696980953216553,
      "learning_rate": 0.0002777335897211782,
      "loss": 1.9375,
      "step": 5100
    },
    {
      "epoch": 0.22725286251201818,
      "grad_norm": 2.8534672260284424,
      "learning_rate": 0.0002772965649855782,
      "loss": 1.9469,
      "step": 5200
    },
    {
      "epoch": 0.23162310986801854,
      "grad_norm": 4.397843360900879,
      "learning_rate": 0.0002768595402499781,
      "loss": 1.8857,
      "step": 5300
    },
    {
      "epoch": 0.23599335722401887,
      "grad_norm": 3.6217305660247803,
      "learning_rate": 0.00027642251551437806,
      "loss": 1.9424,
      "step": 5400
    },
    {
      "epoch": 0.24036360458001924,
      "grad_norm": 3.4536333084106445,
      "learning_rate": 0.00027598549077877804,
      "loss": 1.7746,
      "step": 5500
    },
    {
      "epoch": 0.24473385193601957,
      "grad_norm": 5.208400726318359,
      "learning_rate": 0.00027554846604317803,
      "loss": 1.9013,
      "step": 5600
    },
    {
      "epoch": 0.24910409929201993,
      "grad_norm": 4.536773204803467,
      "learning_rate": 0.00027511144130757797,
      "loss": 1.8311,
      "step": 5700
    },
    {
      "epoch": 0.25347434664802027,
      "grad_norm": 3.1666758060455322,
      "learning_rate": 0.00027467441657197796,
      "loss": 1.9866,
      "step": 5800
    },
    {
      "epoch": 0.25784459400402066,
      "grad_norm": 3.8309056758880615,
      "learning_rate": 0.0002742373918363779,
      "loss": 1.8259,
      "step": 5900
    },
    {
      "epoch": 0.262214841360021,
      "grad_norm": 4.968215465545654,
      "learning_rate": 0.0002738003671007779,
      "loss": 1.9056,
      "step": 6000
    },
    {
      "epoch": 0.2665850887160213,
      "grad_norm": 7.612511157989502,
      "learning_rate": 0.00027336334236517787,
      "loss": 1.8923,
      "step": 6100
    },
    {
      "epoch": 0.27095533607202166,
      "grad_norm": 5.292689323425293,
      "learning_rate": 0.0002729263176295778,
      "loss": 1.9653,
      "step": 6200
    },
    {
      "epoch": 0.27532558342802205,
      "grad_norm": 4.742517948150635,
      "learning_rate": 0.0002724892928939778,
      "loss": 1.8725,
      "step": 6300
    },
    {
      "epoch": 0.2796958307840224,
      "grad_norm": 5.970901966094971,
      "learning_rate": 0.0002720522681583777,
      "loss": 1.9222,
      "step": 6400
    },
    {
      "epoch": 0.2840660781400227,
      "grad_norm": 3.949087142944336,
      "learning_rate": 0.0002716152434227777,
      "loss": 1.876,
      "step": 6500
    },
    {
      "epoch": 0.28843632549602305,
      "grad_norm": 4.005329608917236,
      "learning_rate": 0.0002711782186871777,
      "loss": 1.8702,
      "step": 6600
    },
    {
      "epoch": 0.29280657285202344,
      "grad_norm": 2.733682870864868,
      "learning_rate": 0.00027074119395157763,
      "loss": 1.8782,
      "step": 6700
    },
    {
      "epoch": 0.2971768202080238,
      "grad_norm": 3.056483745574951,
      "learning_rate": 0.0002703085394633336,
      "loss": 1.8491,
      "step": 6800
    },
    {
      "epoch": 0.3015470675640241,
      "grad_norm": 2.814767360687256,
      "learning_rate": 0.00026987151472773356,
      "loss": 1.8332,
      "step": 6900
    },
    {
      "epoch": 0.3059173149200245,
      "grad_norm": 5.484013080596924,
      "learning_rate": 0.00026943448999213355,
      "loss": 1.8578,
      "step": 7000
    },
    {
      "epoch": 0.31028756227602483,
      "grad_norm": 2.2657318115234375,
      "learning_rate": 0.0002689974652565335,
      "loss": 1.8097,
      "step": 7100
    },
    {
      "epoch": 0.31465780963202516,
      "grad_norm": 2.738464832305908,
      "learning_rate": 0.00026856044052093347,
      "loss": 1.9061,
      "step": 7200
    },
    {
      "epoch": 0.3190280569880255,
      "grad_norm": 4.389987468719482,
      "learning_rate": 0.00026812341578533346,
      "loss": 1.8935,
      "step": 7300
    },
    {
      "epoch": 0.3233983043440259,
      "grad_norm": 4.195698261260986,
      "learning_rate": 0.0002676863910497334,
      "loss": 1.8394,
      "step": 7400
    },
    {
      "epoch": 0.3277685517000262,
      "grad_norm": 3.5580880641937256,
      "learning_rate": 0.0002672493663141334,
      "loss": 1.8241,
      "step": 7500
    },
    {
      "epoch": 0.33213879905602656,
      "grad_norm": 3.8002429008483887,
      "learning_rate": 0.0002668123415785333,
      "loss": 1.8464,
      "step": 7600
    },
    {
      "epoch": 0.33650904641202695,
      "grad_norm": 4.831849098205566,
      "learning_rate": 0.00026637531684293325,
      "loss": 1.8169,
      "step": 7700
    },
    {
      "epoch": 0.3408792937680273,
      "grad_norm": 2.8967671394348145,
      "learning_rate": 0.00026593829210733324,
      "loss": 1.8011,
      "step": 7800
    },
    {
      "epoch": 0.3452495411240276,
      "grad_norm": 3.7623558044433594,
      "learning_rate": 0.0002655012673717332,
      "loss": 1.7998,
      "step": 7900
    },
    {
      "epoch": 0.34961978848002795,
      "grad_norm": 5.200533390045166,
      "learning_rate": 0.00026506424263613316,
      "loss": 1.7919,
      "step": 8000
    },
    {
      "epoch": 0.35399003583602834,
      "grad_norm": 2.8523290157318115,
      "learning_rate": 0.00026463158814788915,
      "loss": 1.7225,
      "step": 8100
    },
    {
      "epoch": 0.3583602831920287,
      "grad_norm": 3.9593684673309326,
      "learning_rate": 0.0002641945634122891,
      "loss": 1.8763,
      "step": 8200
    },
    {
      "epoch": 0.362730530548029,
      "grad_norm": 3.577695846557617,
      "learning_rate": 0.00026375753867668907,
      "loss": 1.8158,
      "step": 8300
    },
    {
      "epoch": 0.36710077790402934,
      "grad_norm": 1.5832418203353882,
      "learning_rate": 0.00026332051394108906,
      "loss": 1.7634,
      "step": 8400
    },
    {
      "epoch": 0.37147102526002973,
      "grad_norm": 3.874330759048462,
      "learning_rate": 0.000262883489205489,
      "loss": 1.8532,
      "step": 8500
    },
    {
      "epoch": 0.37584127261603006,
      "grad_norm": 4.183368682861328,
      "learning_rate": 0.000262446464469889,
      "loss": 1.8048,
      "step": 8600
    },
    {
      "epoch": 0.3802115199720304,
      "grad_norm": 5.342069625854492,
      "learning_rate": 0.0002620094397342889,
      "loss": 1.7676,
      "step": 8700
    },
    {
      "epoch": 0.3845817673280308,
      "grad_norm": 3.2436845302581787,
      "learning_rate": 0.0002615724149986889,
      "loss": 1.7692,
      "step": 8800
    },
    {
      "epoch": 0.3889520146840311,
      "grad_norm": 6.646078109741211,
      "learning_rate": 0.0002611353902630889,
      "loss": 1.8797,
      "step": 8900
    },
    {
      "epoch": 0.39332226204003146,
      "grad_norm": 5.3593339920043945,
      "learning_rate": 0.00026069836552748883,
      "loss": 1.7804,
      "step": 9000
    },
    {
      "epoch": 0.3976925093960318,
      "grad_norm": 3.682811975479126,
      "learning_rate": 0.0002602613407918888,
      "loss": 1.7959,
      "step": 9100
    },
    {
      "epoch": 0.4020627567520322,
      "grad_norm": 3.7135820388793945,
      "learning_rate": 0.00025982431605628875,
      "loss": 1.7647,
      "step": 9200
    },
    {
      "epoch": 0.4064330041080325,
      "grad_norm": 1.9626057147979736,
      "learning_rate": 0.00025938729132068874,
      "loss": 1.8245,
      "step": 9300
    },
    {
      "epoch": 0.41080325146403285,
      "grad_norm": 2.6538455486297607,
      "learning_rate": 0.0002589502665850887,
      "loss": 1.8045,
      "step": 9400
    },
    {
      "epoch": 0.41517349882003324,
      "grad_norm": 3.334244966506958,
      "learning_rate": 0.00025851324184948866,
      "loss": 1.8173,
      "step": 9500
    },
    {
      "epoch": 0.41954374617603357,
      "grad_norm": 3.754650115966797,
      "learning_rate": 0.00025807621711388865,
      "loss": 1.8203,
      "step": 9600
    },
    {
      "epoch": 0.4239139935320339,
      "grad_norm": 3.557469129562378,
      "learning_rate": 0.0002576391923782886,
      "loss": 1.6884,
      "step": 9700
    },
    {
      "epoch": 0.42828424088803424,
      "grad_norm": 5.099214553833008,
      "learning_rate": 0.0002572021676426885,
      "loss": 1.7944,
      "step": 9800
    },
    {
      "epoch": 0.43265448824403463,
      "grad_norm": 1.818070411682129,
      "learning_rate": 0.0002567651429070885,
      "loss": 1.8001,
      "step": 9900
    },
    {
      "epoch": 0.43702473560003496,
      "grad_norm": 4.100385665893555,
      "learning_rate": 0.0002563281181714885,
      "loss": 1.8054,
      "step": 10000
    },
    {
      "epoch": 0.4413949829560353,
      "grad_norm": 6.629307270050049,
      "learning_rate": 0.00025589109343588843,
      "loss": 1.7892,
      "step": 10100
    },
    {
      "epoch": 0.4457652303120357,
      "grad_norm": 3.4661991596221924,
      "learning_rate": 0.0002554540687002884,
      "loss": 1.7663,
      "step": 10200
    },
    {
      "epoch": 0.450135477668036,
      "grad_norm": 2.9376931190490723,
      "learning_rate": 0.00025501704396468835,
      "loss": 1.7156,
      "step": 10300
    },
    {
      "epoch": 0.45450572502403636,
      "grad_norm": 3.4059829711914062,
      "learning_rate": 0.00025458001922908834,
      "loss": 1.8419,
      "step": 10400
    },
    {
      "epoch": 0.4588759723800367,
      "grad_norm": 3.2544846534729004,
      "learning_rate": 0.0002541429944934883,
      "loss": 1.8065,
      "step": 10500
    },
    {
      "epoch": 0.4632462197360371,
      "grad_norm": 3.71806001663208,
      "learning_rate": 0.00025370596975788826,
      "loss": 1.7933,
      "step": 10600
    },
    {
      "epoch": 0.4676164670920374,
      "grad_norm": 2.5262067317962646,
      "learning_rate": 0.00025326894502228825,
      "loss": 1.7727,
      "step": 10700
    },
    {
      "epoch": 0.47198671444803775,
      "grad_norm": 3.2766833305358887,
      "learning_rate": 0.0002528319202866882,
      "loss": 1.8364,
      "step": 10800
    },
    {
      "epoch": 0.4763569618040381,
      "grad_norm": 4.028967380523682,
      "learning_rate": 0.00025239489555108817,
      "loss": 1.8256,
      "step": 10900
    },
    {
      "epoch": 0.48072720916003847,
      "grad_norm": 3.4150784015655518,
      "learning_rate": 0.00025195787081548816,
      "loss": 1.8383,
      "step": 11000
    },
    {
      "epoch": 0.4850974565160388,
      "grad_norm": 3.1540846824645996,
      "learning_rate": 0.0002515208460798881,
      "loss": 1.7965,
      "step": 11100
    },
    {
      "epoch": 0.48946770387203914,
      "grad_norm": 2.6063246726989746,
      "learning_rate": 0.0002510838213442881,
      "loss": 1.7861,
      "step": 11200
    },
    {
      "epoch": 0.49383795122803953,
      "grad_norm": 3.0409183502197266,
      "learning_rate": 0.000250646796608688,
      "loss": 1.8036,
      "step": 11300
    },
    {
      "epoch": 0.49820819858403986,
      "grad_norm": 3.7213499546051025,
      "learning_rate": 0.000250209771873088,
      "loss": 1.7379,
      "step": 11400
    },
    {
      "epoch": 0.5025784459400402,
      "grad_norm": 1.9178036451339722,
      "learning_rate": 0.000249772747137488,
      "loss": 1.7395,
      "step": 11500
    },
    {
      "epoch": 0.5069486932960405,
      "grad_norm": 3.875641345977783,
      "learning_rate": 0.0002493357224018879,
      "loss": 1.7981,
      "step": 11600
    },
    {
      "epoch": 0.5113189406520409,
      "grad_norm": 2.752086639404297,
      "learning_rate": 0.0002488986976662879,
      "loss": 1.7585,
      "step": 11700
    },
    {
      "epoch": 0.5156891880080413,
      "grad_norm": 3.9201955795288086,
      "learning_rate": 0.00024846167293068785,
      "loss": 1.6816,
      "step": 11800
    },
    {
      "epoch": 0.5200594353640416,
      "grad_norm": 1.2277010679244995,
      "learning_rate": 0.0002480246481950878,
      "loss": 1.76,
      "step": 11900
    },
    {
      "epoch": 0.524429682720042,
      "grad_norm": 3.6041505336761475,
      "learning_rate": 0.00024758762345948777,
      "loss": 1.7879,
      "step": 12000
    },
    {
      "epoch": 0.5287999300760423,
      "grad_norm": 3.3168442249298096,
      "learning_rate": 0.00024715059872388776,
      "loss": 1.7459,
      "step": 12100
    },
    {
      "epoch": 0.5331701774320426,
      "grad_norm": 2.0685958862304688,
      "learning_rate": 0.0002467135739882877,
      "loss": 1.729,
      "step": 12200
    },
    {
      "epoch": 0.537540424788043,
      "grad_norm": 4.047937870025635,
      "learning_rate": 0.0002462765492526877,
      "loss": 1.7193,
      "step": 12300
    },
    {
      "epoch": 0.5419106721440433,
      "grad_norm": 2.9879794120788574,
      "learning_rate": 0.0002458395245170876,
      "loss": 1.7589,
      "step": 12400
    },
    {
      "epoch": 0.5462809195000436,
      "grad_norm": 3.979149580001831,
      "learning_rate": 0.0002454024997814876,
      "loss": 1.8153,
      "step": 12500
    },
    {
      "epoch": 0.5506511668560441,
      "grad_norm": 1.8085137605667114,
      "learning_rate": 0.0002449654750458876,
      "loss": 1.6844,
      "step": 12600
    },
    {
      "epoch": 0.5550214142120444,
      "grad_norm": 3.2640328407287598,
      "learning_rate": 0.0002445284503102875,
      "loss": 1.7096,
      "step": 12700
    },
    {
      "epoch": 0.5593916615680448,
      "grad_norm": 3.0720314979553223,
      "learning_rate": 0.00024409142557468751,
      "loss": 1.7775,
      "step": 12800
    },
    {
      "epoch": 0.5637619089240451,
      "grad_norm": 2.2983062267303467,
      "learning_rate": 0.00024365440083908748,
      "loss": 1.7871,
      "step": 12900
    },
    {
      "epoch": 0.5681321562800454,
      "grad_norm": 2.8604650497436523,
      "learning_rate": 0.00024321737610348744,
      "loss": 1.6828,
      "step": 13000
    },
    {
      "epoch": 0.5725024036360458,
      "grad_norm": 5.9980058670043945,
      "learning_rate": 0.0002427803513678874,
      "loss": 1.7134,
      "step": 13100
    },
    {
      "epoch": 0.5768726509920461,
      "grad_norm": 3.694218873977661,
      "learning_rate": 0.00024234332663228736,
      "loss": 1.8179,
      "step": 13200
    },
    {
      "epoch": 0.5812428983480465,
      "grad_norm": 2.7880215644836426,
      "learning_rate": 0.00024190630189668735,
      "loss": 1.6889,
      "step": 13300
    },
    {
      "epoch": 0.5856131457040469,
      "grad_norm": 4.953062534332275,
      "learning_rate": 0.0002414692771610873,
      "loss": 1.7508,
      "step": 13400
    },
    {
      "epoch": 0.5899833930600472,
      "grad_norm": 3.4736971855163574,
      "learning_rate": 0.00024103225242548727,
      "loss": 1.7364,
      "step": 13500
    },
    {
      "epoch": 0.5943536404160475,
      "grad_norm": 3.261990547180176,
      "learning_rate": 0.00024059522768988723,
      "loss": 1.8424,
      "step": 13600
    },
    {
      "epoch": 0.5987238877720479,
      "grad_norm": 4.616734504699707,
      "learning_rate": 0.0002401582029542872,
      "loss": 1.7464,
      "step": 13700
    },
    {
      "epoch": 0.6030941351280482,
      "grad_norm": 2.7609176635742188,
      "learning_rate": 0.00023972117821868718,
      "loss": 1.769,
      "step": 13800
    },
    {
      "epoch": 0.6074643824840485,
      "grad_norm": 5.149203300476074,
      "learning_rate": 0.00023928415348308714,
      "loss": 1.6918,
      "step": 13900
    },
    {
      "epoch": 0.611834629840049,
      "grad_norm": 1.953402042388916,
      "learning_rate": 0.00023884712874748708,
      "loss": 1.7233,
      "step": 14000
    },
    {
      "epoch": 0.6162048771960493,
      "grad_norm": 2.189389228820801,
      "learning_rate": 0.00023841010401188704,
      "loss": 1.6938,
      "step": 14100
    },
    {
      "epoch": 0.6205751245520497,
      "grad_norm": 4.002466201782227,
      "learning_rate": 0.000237977449523643,
      "loss": 1.6927,
      "step": 14200
    },
    {
      "epoch": 0.62494537190805,
      "grad_norm": 2.600496292114258,
      "learning_rate": 0.00023754042478804296,
      "loss": 1.7272,
      "step": 14300
    },
    {
      "epoch": 0.6293156192640503,
      "grad_norm": 3.405022382736206,
      "learning_rate": 0.00023710777029979893,
      "loss": 1.7621,
      "step": 14400
    },
    {
      "epoch": 0.6336858666200507,
      "grad_norm": 4.096743583679199,
      "learning_rate": 0.00023667074556419892,
      "loss": 1.7414,
      "step": 14500
    },
    {
      "epoch": 0.638056113976051,
      "grad_norm": 3.301764726638794,
      "learning_rate": 0.00023623372082859888,
      "loss": 1.7323,
      "step": 14600
    },
    {
      "epoch": 0.6424263613320514,
      "grad_norm": 4.331884384155273,
      "learning_rate": 0.00023579669609299884,
      "loss": 1.6723,
      "step": 14700
    },
    {
      "epoch": 0.6467966086880518,
      "grad_norm": 3.1484198570251465,
      "learning_rate": 0.0002353596713573988,
      "loss": 1.6993,
      "step": 14800
    },
    {
      "epoch": 0.6511668560440521,
      "grad_norm": 4.091912746429443,
      "learning_rate": 0.00023492264662179876,
      "loss": 1.7143,
      "step": 14900
    },
    {
      "epoch": 0.6555371034000524,
      "grad_norm": 3.2229018211364746,
      "learning_rate": 0.00023448562188619875,
      "loss": 1.6593,
      "step": 15000
    },
    {
      "epoch": 0.6599073507560528,
      "grad_norm": 3.586552143096924,
      "learning_rate": 0.0002340485971505987,
      "loss": 1.7099,
      "step": 15100
    },
    {
      "epoch": 0.6642775981120531,
      "grad_norm": 4.642912864685059,
      "learning_rate": 0.00023361157241499867,
      "loss": 1.7622,
      "step": 15200
    },
    {
      "epoch": 0.6686478454680534,
      "grad_norm": 3.388275146484375,
      "learning_rate": 0.00023317454767939863,
      "loss": 1.728,
      "step": 15300
    },
    {
      "epoch": 0.6730180928240539,
      "grad_norm": 3.2557578086853027,
      "learning_rate": 0.0002327375229437986,
      "loss": 1.6682,
      "step": 15400
    },
    {
      "epoch": 0.6773883401800542,
      "grad_norm": 2.77970814704895,
      "learning_rate": 0.00023230049820819858,
      "loss": 1.7257,
      "step": 15500
    },
    {
      "epoch": 0.6817585875360546,
      "grad_norm": 4.585293769836426,
      "learning_rate": 0.00023186347347259854,
      "loss": 1.8325,
      "step": 15600
    },
    {
      "epoch": 0.6861288348920549,
      "grad_norm": 3.3158934116363525,
      "learning_rate": 0.0002314264487369985,
      "loss": 1.7137,
      "step": 15700
    },
    {
      "epoch": 0.6904990822480552,
      "grad_norm": 2.403367280960083,
      "learning_rate": 0.00023098942400139847,
      "loss": 1.6934,
      "step": 15800
    },
    {
      "epoch": 0.6948693296040556,
      "grad_norm": 2.7843077182769775,
      "learning_rate": 0.00023055239926579843,
      "loss": 1.7178,
      "step": 15900
    },
    {
      "epoch": 0.6992395769600559,
      "grad_norm": 3.1140122413635254,
      "learning_rate": 0.00023011537453019841,
      "loss": 1.6539,
      "step": 16000
    },
    {
      "epoch": 0.7036098243160563,
      "grad_norm": 4.115045547485352,
      "learning_rate": 0.00022967834979459835,
      "loss": 1.7374,
      "step": 16100
    },
    {
      "epoch": 0.7079800716720567,
      "grad_norm": 1.9667164087295532,
      "learning_rate": 0.0002292413250589983,
      "loss": 1.7431,
      "step": 16200
    },
    {
      "epoch": 0.712350319028057,
      "grad_norm": 4.958922863006592,
      "learning_rate": 0.00022880430032339827,
      "loss": 1.6239,
      "step": 16300
    },
    {
      "epoch": 0.7167205663840573,
      "grad_norm": 3.482067823410034,
      "learning_rate": 0.00022836727558779823,
      "loss": 1.6329,
      "step": 16400
    },
    {
      "epoch": 0.7210908137400577,
      "grad_norm": 8.057339668273926,
      "learning_rate": 0.0002279302508521982,
      "loss": 1.655,
      "step": 16500
    },
    {
      "epoch": 0.725461061096058,
      "grad_norm": 2.3308093547821045,
      "learning_rate": 0.00022749759636395416,
      "loss": 1.7756,
      "step": 16600
    },
    {
      "epoch": 0.7298313084520583,
      "grad_norm": 3.561619281768799,
      "learning_rate": 0.00022706057162835415,
      "loss": 1.6929,
      "step": 16700
    },
    {
      "epoch": 0.7342015558080587,
      "grad_norm": 4.411243915557861,
      "learning_rate": 0.0002266279171401101,
      "loss": 1.6686,
      "step": 16800
    },
    {
      "epoch": 0.7385718031640591,
      "grad_norm": 4.348694324493408,
      "learning_rate": 0.00022619089240451007,
      "loss": 1.6864,
      "step": 16900
    },
    {
      "epoch": 0.7429420505200595,
      "grad_norm": 4.989090919494629,
      "learning_rate": 0.00022575386766891003,
      "loss": 1.7816,
      "step": 17000
    },
    {
      "epoch": 0.7473122978760598,
      "grad_norm": 3.6665468215942383,
      "learning_rate": 0.00022531684293331,
      "loss": 1.6658,
      "step": 17100
    },
    {
      "epoch": 0.7516825452320601,
      "grad_norm": 3.9684321880340576,
      "learning_rate": 0.00022487981819770998,
      "loss": 1.7205,
      "step": 17200
    },
    {
      "epoch": 0.7560527925880605,
      "grad_norm": 1.0106149911880493,
      "learning_rate": 0.00022444279346210994,
      "loss": 1.6352,
      "step": 17300
    },
    {
      "epoch": 0.7604230399440608,
      "grad_norm": 3.2467024326324463,
      "learning_rate": 0.0002240057687265099,
      "loss": 1.7158,
      "step": 17400
    },
    {
      "epoch": 0.7647932873000611,
      "grad_norm": 3.91776967048645,
      "learning_rate": 0.00022356874399090987,
      "loss": 1.6843,
      "step": 17500
    },
    {
      "epoch": 0.7691635346560616,
      "grad_norm": 2.716087818145752,
      "learning_rate": 0.00022313171925530983,
      "loss": 1.7446,
      "step": 17600
    },
    {
      "epoch": 0.7735337820120619,
      "grad_norm": 4.94862699508667,
      "learning_rate": 0.00022269469451970982,
      "loss": 1.6358,
      "step": 17700
    },
    {
      "epoch": 0.7779040293680622,
      "grad_norm": 2.800309419631958,
      "learning_rate": 0.00022225766978410978,
      "loss": 1.7316,
      "step": 17800
    },
    {
      "epoch": 0.7822742767240626,
      "grad_norm": 6.016802787780762,
      "learning_rate": 0.00022182064504850974,
      "loss": 1.7609,
      "step": 17900
    },
    {
      "epoch": 0.7866445240800629,
      "grad_norm": 3.3475632667541504,
      "learning_rate": 0.0002213836203129097,
      "loss": 1.641,
      "step": 18000
    },
    {
      "epoch": 0.7910147714360632,
      "grad_norm": 8.804244995117188,
      "learning_rate": 0.00022094659557730966,
      "loss": 1.6672,
      "step": 18100
    },
    {
      "epoch": 0.7953850187920636,
      "grad_norm": 3.0273513793945312,
      "learning_rate": 0.00022050957084170965,
      "loss": 1.6441,
      "step": 18200
    },
    {
      "epoch": 0.799755266148064,
      "grad_norm": 3.5781896114349365,
      "learning_rate": 0.00022007254610610958,
      "loss": 1.6443,
      "step": 18300
    },
    {
      "epoch": 0.8041255135040644,
      "grad_norm": 2.417830228805542,
      "learning_rate": 0.00021963552137050954,
      "loss": 1.6946,
      "step": 18400
    },
    {
      "epoch": 0.8084957608600647,
      "grad_norm": 4.349771976470947,
      "learning_rate": 0.0002191984966349095,
      "loss": 1.6428,
      "step": 18500
    },
    {
      "epoch": 0.812866008216065,
      "grad_norm": 3.644026279449463,
      "learning_rate": 0.00021876147189930947,
      "loss": 1.6602,
      "step": 18600
    },
    {
      "epoch": 0.8172362555720654,
      "grad_norm": 3.463766098022461,
      "learning_rate": 0.00021832444716370943,
      "loss": 1.79,
      "step": 18700
    },
    {
      "epoch": 0.8216065029280657,
      "grad_norm": 4.495893955230713,
      "learning_rate": 0.00021788742242810942,
      "loss": 1.6135,
      "step": 18800
    },
    {
      "epoch": 0.825976750284066,
      "grad_norm": 3.9348111152648926,
      "learning_rate": 0.00021745039769250938,
      "loss": 1.7897,
      "step": 18900
    },
    {
      "epoch": 0.8303469976400665,
      "grad_norm": 3.283750057220459,
      "learning_rate": 0.00021701337295690934,
      "loss": 1.7632,
      "step": 19000
    },
    {
      "epoch": 0.8347172449960668,
      "grad_norm": 3.069603204727173,
      "learning_rate": 0.0002165763482213093,
      "loss": 1.7153,
      "step": 19100
    },
    {
      "epoch": 0.8390874923520671,
      "grad_norm": 3.674447774887085,
      "learning_rate": 0.00021613932348570926,
      "loss": 1.8025,
      "step": 19200
    },
    {
      "epoch": 0.8434577397080675,
      "grad_norm": 3.982984781265259,
      "learning_rate": 0.00021570229875010925,
      "loss": 1.6923,
      "step": 19300
    },
    {
      "epoch": 0.8478279870640678,
      "grad_norm": 4.445491790771484,
      "learning_rate": 0.0002152652740145092,
      "loss": 1.6956,
      "step": 19400
    },
    {
      "epoch": 0.8521982344200681,
      "grad_norm": 1.9064561128616333,
      "learning_rate": 0.00021482824927890917,
      "loss": 1.6989,
      "step": 19500
    },
    {
      "epoch": 0.8565684817760685,
      "grad_norm": 3.497422933578491,
      "learning_rate": 0.00021439122454330913,
      "loss": 1.7153,
      "step": 19600
    },
    {
      "epoch": 0.8609387291320689,
      "grad_norm": 3.6018025875091553,
      "learning_rate": 0.0002139541998077091,
      "loss": 1.6288,
      "step": 19700
    },
    {
      "epoch": 0.8653089764880693,
      "grad_norm": 4.1417741775512695,
      "learning_rate": 0.00021351717507210908,
      "loss": 1.6709,
      "step": 19800
    },
    {
      "epoch": 0.8696792238440696,
      "grad_norm": 3.7082927227020264,
      "learning_rate": 0.00021308015033650904,
      "loss": 1.6042,
      "step": 19900
    },
    {
      "epoch": 0.8740494712000699,
      "grad_norm": 3.9073314666748047,
      "learning_rate": 0.000212643125600909,
      "loss": 1.7595,
      "step": 20000
    },
    {
      "epoch": 0.8784197185560703,
      "grad_norm": 1.779976725578308,
      "learning_rate": 0.00021220610086530896,
      "loss": 1.615,
      "step": 20100
    },
    {
      "epoch": 0.8827899659120706,
      "grad_norm": 3.678520441055298,
      "learning_rate": 0.00021176907612970893,
      "loss": 1.6423,
      "step": 20200
    },
    {
      "epoch": 0.8871602132680709,
      "grad_norm": 4.437901496887207,
      "learning_rate": 0.00021133205139410891,
      "loss": 1.7858,
      "step": 20300
    },
    {
      "epoch": 0.8915304606240714,
      "grad_norm": 2.3899824619293213,
      "learning_rate": 0.00021089502665850885,
      "loss": 1.7074,
      "step": 20400
    },
    {
      "epoch": 0.8959007079800717,
      "grad_norm": 4.934350490570068,
      "learning_rate": 0.0002104580019229088,
      "loss": 1.668,
      "step": 20500
    },
    {
      "epoch": 0.900270955336072,
      "grad_norm": 3.863593816757202,
      "learning_rate": 0.00021002097718730877,
      "loss": 1.6929,
      "step": 20600
    },
    {
      "epoch": 0.9046412026920724,
      "grad_norm": 3.3118903636932373,
      "learning_rate": 0.00020958395245170873,
      "loss": 1.6593,
      "step": 20700
    },
    {
      "epoch": 0.9090114500480727,
      "grad_norm": 4.852874279022217,
      "learning_rate": 0.0002091469277161087,
      "loss": 1.7037,
      "step": 20800
    },
    {
      "epoch": 0.913381697404073,
      "grad_norm": 3.9022622108459473,
      "learning_rate": 0.00020870990298050868,
      "loss": 1.6958,
      "step": 20900
    },
    {
      "epoch": 0.9177519447600734,
      "grad_norm": 3.266871690750122,
      "learning_rate": 0.00020827287824490864,
      "loss": 1.6585,
      "step": 21000
    },
    {
      "epoch": 0.9221221921160738,
      "grad_norm": 3.8679401874542236,
      "learning_rate": 0.0002078358535093086,
      "loss": 1.7296,
      "step": 21100
    },
    {
      "epoch": 0.9264924394720742,
      "grad_norm": 1.718543529510498,
      "learning_rate": 0.00020739882877370856,
      "loss": 1.5609,
      "step": 21200
    },
    {
      "epoch": 0.9308626868280745,
      "grad_norm": 5.936127662658691,
      "learning_rate": 0.00020696180403810853,
      "loss": 1.7753,
      "step": 21300
    },
    {
      "epoch": 0.9352329341840748,
      "grad_norm": 2.195607900619507,
      "learning_rate": 0.00020652477930250851,
      "loss": 1.6433,
      "step": 21400
    },
    {
      "epoch": 0.9396031815400752,
      "grad_norm": 2.5223095417022705,
      "learning_rate": 0.00020608775456690848,
      "loss": 1.7256,
      "step": 21500
    },
    {
      "epoch": 0.9439734288960755,
      "grad_norm": 4.511275291442871,
      "learning_rate": 0.00020565072983130844,
      "loss": 1.7521,
      "step": 21600
    },
    {
      "epoch": 0.9483436762520758,
      "grad_norm": 3.27905535697937,
      "learning_rate": 0.0002052137050957084,
      "loss": 1.7479,
      "step": 21700
    },
    {
      "epoch": 0.9527139236080762,
      "grad_norm": 2.0794906616210938,
      "learning_rate": 0.00020477668036010836,
      "loss": 1.6399,
      "step": 21800
    },
    {
      "epoch": 0.9570841709640766,
      "grad_norm": 3.1746065616607666,
      "learning_rate": 0.00020433965562450835,
      "loss": 1.5618,
      "step": 21900
    },
    {
      "epoch": 0.9614544183200769,
      "grad_norm": 1.208117127418518,
      "learning_rate": 0.0002039026308889083,
      "loss": 1.676,
      "step": 22000
    },
    {
      "epoch": 0.9658246656760773,
      "grad_norm": 1.6072839498519897,
      "learning_rate": 0.00020346560615330827,
      "loss": 1.6201,
      "step": 22100
    },
    {
      "epoch": 0.9701949130320776,
      "grad_norm": 2.2275397777557373,
      "learning_rate": 0.00020302858141770823,
      "loss": 1.687,
      "step": 22200
    },
    {
      "epoch": 0.974565160388078,
      "grad_norm": 1.549126148223877,
      "learning_rate": 0.0002025915566821082,
      "loss": 1.6212,
      "step": 22300
    },
    {
      "epoch": 0.9789354077440783,
      "grad_norm": 5.917268753051758,
      "learning_rate": 0.00020215453194650818,
      "loss": 1.6104,
      "step": 22400
    },
    {
      "epoch": 0.9833056551000786,
      "grad_norm": 2.935333013534546,
      "learning_rate": 0.00020171750721090814,
      "loss": 1.6537,
      "step": 22500
    },
    {
      "epoch": 0.9876759024560791,
      "grad_norm": 3.1291229724884033,
      "learning_rate": 0.00020128048247530808,
      "loss": 1.6315,
      "step": 22600
    },
    {
      "epoch": 0.9920461498120794,
      "grad_norm": 4.2515668869018555,
      "learning_rate": 0.00020084345773970804,
      "loss": 1.6773,
      "step": 22700
    },
    {
      "epoch": 0.9964163971680797,
      "grad_norm": 3.314440965652466,
      "learning_rate": 0.000200406433004108,
      "loss": 1.7285,
      "step": 22800
    },
    {
      "epoch": 1.00078664452408,
      "grad_norm": 2.9887702465057373,
      "learning_rate": 0.00019996940826850796,
      "loss": 1.7262,
      "step": 22900
    },
    {
      "epoch": 1.0051568918800804,
      "grad_norm": 5.844688892364502,
      "learning_rate": 0.00019953675378026392,
      "loss": 1.622,
      "step": 23000
    },
    {
      "epoch": 1.0095271392360807,
      "grad_norm": 4.104518890380859,
      "learning_rate": 0.0001990997290446639,
      "loss": 1.7344,
      "step": 23100
    },
    {
      "epoch": 1.013897386592081,
      "grad_norm": 4.58437967300415,
      "learning_rate": 0.00019866270430906387,
      "loss": 1.681,
      "step": 23200
    },
    {
      "epoch": 1.0182676339480814,
      "grad_norm": 3.5118725299835205,
      "learning_rate": 0.00019822567957346383,
      "loss": 1.66,
      "step": 23300
    },
    {
      "epoch": 1.0226378813040817,
      "grad_norm": 6.337378025054932,
      "learning_rate": 0.0001977886548378638,
      "loss": 1.6936,
      "step": 23400
    },
    {
      "epoch": 1.027008128660082,
      "grad_norm": 3.467888593673706,
      "learning_rate": 0.00019735163010226376,
      "loss": 1.6512,
      "step": 23500
    },
    {
      "epoch": 1.0313783760160824,
      "grad_norm": 4.2581305503845215,
      "learning_rate": 0.00019691460536666374,
      "loss": 1.6421,
      "step": 23600
    },
    {
      "epoch": 1.035748623372083,
      "grad_norm": 2.3157734870910645,
      "learning_rate": 0.0001964775806310637,
      "loss": 1.6418,
      "step": 23700
    },
    {
      "epoch": 1.0401188707280833,
      "grad_norm": 3.6007611751556396,
      "learning_rate": 0.00019604055589546367,
      "loss": 1.6147,
      "step": 23800
    },
    {
      "epoch": 1.0444891180840836,
      "grad_norm": 4.108325004577637,
      "learning_rate": 0.00019560353115986363,
      "loss": 1.6359,
      "step": 23900
    },
    {
      "epoch": 1.048859365440084,
      "grad_norm": 7.412835121154785,
      "learning_rate": 0.0001951665064242636,
      "loss": 1.6372,
      "step": 24000
    },
    {
      "epoch": 1.0532296127960843,
      "grad_norm": 4.557158946990967,
      "learning_rate": 0.00019472948168866358,
      "loss": 1.6069,
      "step": 24100
    },
    {
      "epoch": 1.0575998601520846,
      "grad_norm": 3.09372878074646,
      "learning_rate": 0.00019429245695306354,
      "loss": 1.6095,
      "step": 24200
    },
    {
      "epoch": 1.061970107508085,
      "grad_norm": 2.7418813705444336,
      "learning_rate": 0.0001938554322174635,
      "loss": 1.611,
      "step": 24300
    },
    {
      "epoch": 1.0663403548640853,
      "grad_norm": 2.386880874633789,
      "learning_rate": 0.00019341840748186346,
      "loss": 1.6315,
      "step": 24400
    },
    {
      "epoch": 1.0707106022200856,
      "grad_norm": 3.174727439880371,
      "learning_rate": 0.00019298138274626342,
      "loss": 1.5816,
      "step": 24500
    },
    {
      "epoch": 1.075080849576086,
      "grad_norm": 1.9364444017410278,
      "learning_rate": 0.0001925443580106634,
      "loss": 1.6568,
      "step": 24600
    },
    {
      "epoch": 1.0794510969320863,
      "grad_norm": 4.585980415344238,
      "learning_rate": 0.00019210733327506334,
      "loss": 1.5012,
      "step": 24700
    },
    {
      "epoch": 1.0838213442880866,
      "grad_norm": 3.9340922832489014,
      "learning_rate": 0.0001916703085394633,
      "loss": 1.6764,
      "step": 24800
    },
    {
      "epoch": 1.088191591644087,
      "grad_norm": 3.158431053161621,
      "learning_rate": 0.00019123328380386327,
      "loss": 1.5798,
      "step": 24900
    },
    {
      "epoch": 1.0925618390000875,
      "grad_norm": 2.373025894165039,
      "learning_rate": 0.00019079625906826323,
      "loss": 1.625,
      "step": 25000
    },
    {
      "epoch": 1.0969320863560879,
      "grad_norm": 2.707714796066284,
      "learning_rate": 0.0001903592343326632,
      "loss": 1.6246,
      "step": 25100
    },
    {
      "epoch": 1.1013023337120882,
      "grad_norm": 1.8065121173858643,
      "learning_rate": 0.00018992657984441918,
      "loss": 1.6931,
      "step": 25200
    },
    {
      "epoch": 1.1056725810680885,
      "grad_norm": 3.727898120880127,
      "learning_rate": 0.00018948955510881914,
      "loss": 1.67,
      "step": 25300
    },
    {
      "epoch": 1.1100428284240889,
      "grad_norm": 4.1211442947387695,
      "learning_rate": 0.0001890525303732191,
      "loss": 1.6488,
      "step": 25400
    },
    {
      "epoch": 1.1144130757800892,
      "grad_norm": 3.161722183227539,
      "learning_rate": 0.00018861550563761906,
      "loss": 1.6585,
      "step": 25500
    },
    {
      "epoch": 1.1187833231360895,
      "grad_norm": 3.6904022693634033,
      "learning_rate": 0.00018817848090201903,
      "loss": 1.6291,
      "step": 25600
    },
    {
      "epoch": 1.1231535704920899,
      "grad_norm": 4.117436408996582,
      "learning_rate": 0.00018774145616641901,
      "loss": 1.6985,
      "step": 25700
    },
    {
      "epoch": 1.1275238178480902,
      "grad_norm": 2.7916932106018066,
      "learning_rate": 0.00018730443143081898,
      "loss": 1.6561,
      "step": 25800
    },
    {
      "epoch": 1.1318940652040905,
      "grad_norm": 2.3588755130767822,
      "learning_rate": 0.00018686740669521894,
      "loss": 1.5767,
      "step": 25900
    },
    {
      "epoch": 1.1362643125600909,
      "grad_norm": 6.594379901885986,
      "learning_rate": 0.0001864303819596189,
      "loss": 1.6251,
      "step": 26000
    },
    {
      "epoch": 1.1406345599160912,
      "grad_norm": 2.6653525829315186,
      "learning_rate": 0.00018599335722401886,
      "loss": 1.7044,
      "step": 26100
    },
    {
      "epoch": 1.1450048072720915,
      "grad_norm": 3.007544994354248,
      "learning_rate": 0.00018555633248841885,
      "loss": 1.6,
      "step": 26200
    },
    {
      "epoch": 1.1493750546280919,
      "grad_norm": 3.459761142730713,
      "learning_rate": 0.0001851193077528188,
      "loss": 1.6291,
      "step": 26300
    },
    {
      "epoch": 1.1537453019840922,
      "grad_norm": 3.5487747192382812,
      "learning_rate": 0.00018468228301721877,
      "loss": 1.6358,
      "step": 26400
    },
    {
      "epoch": 1.1581155493400925,
      "grad_norm": 3.873746395111084,
      "learning_rate": 0.00018424525828161873,
      "loss": 1.5854,
      "step": 26500
    },
    {
      "epoch": 1.162485796696093,
      "grad_norm": 4.00846004486084,
      "learning_rate": 0.0001838082335460187,
      "loss": 1.6277,
      "step": 26600
    },
    {
      "epoch": 1.1668560440520934,
      "grad_norm": 3.058399200439453,
      "learning_rate": 0.00018337120881041868,
      "loss": 1.6213,
      "step": 26700
    },
    {
      "epoch": 1.1712262914080938,
      "grad_norm": 3.2880642414093018,
      "learning_rate": 0.00018293418407481864,
      "loss": 1.6411,
      "step": 26800
    },
    {
      "epoch": 1.175596538764094,
      "grad_norm": 3.3683054447174072,
      "learning_rate": 0.00018249715933921858,
      "loss": 1.6661,
      "step": 26900
    },
    {
      "epoch": 1.1799667861200944,
      "grad_norm": 2.544874668121338,
      "learning_rate": 0.00018206013460361854,
      "loss": 1.564,
      "step": 27000
    },
    {
      "epoch": 1.1843370334760948,
      "grad_norm": 3.2555272579193115,
      "learning_rate": 0.0001816231098680185,
      "loss": 1.5839,
      "step": 27100
    },
    {
      "epoch": 1.188707280832095,
      "grad_norm": 3.196585178375244,
      "learning_rate": 0.00018119045537977446,
      "loss": 1.5753,
      "step": 27200
    },
    {
      "epoch": 1.1930775281880954,
      "grad_norm": 3.5726547241210938,
      "learning_rate": 0.00018075343064417442,
      "loss": 1.6191,
      "step": 27300
    },
    {
      "epoch": 1.1974477755440958,
      "grad_norm": 4.412038326263428,
      "learning_rate": 0.0001803164059085744,
      "loss": 1.5545,
      "step": 27400
    },
    {
      "epoch": 1.201818022900096,
      "grad_norm": 5.430164337158203,
      "learning_rate": 0.00017987938117297437,
      "loss": 1.6215,
      "step": 27500
    },
    {
      "epoch": 1.2061882702560964,
      "grad_norm": 4.072381973266602,
      "learning_rate": 0.00017944235643737433,
      "loss": 1.6368,
      "step": 27600
    },
    {
      "epoch": 1.2105585176120968,
      "grad_norm": 2.6409270763397217,
      "learning_rate": 0.0001790053317017743,
      "loss": 1.5772,
      "step": 27700
    },
    {
      "epoch": 1.2149287649680973,
      "grad_norm": 2.9931514263153076,
      "learning_rate": 0.00017856830696617426,
      "loss": 1.6409,
      "step": 27800
    },
    {
      "epoch": 1.2192990123240977,
      "grad_norm": 2.6464593410491943,
      "learning_rate": 0.00017813128223057424,
      "loss": 1.5883,
      "step": 27900
    },
    {
      "epoch": 1.223669259680098,
      "grad_norm": 3.430060863494873,
      "learning_rate": 0.0001776942574949742,
      "loss": 1.6532,
      "step": 28000
    },
    {
      "epoch": 1.2280395070360983,
      "grad_norm": 5.6517415046691895,
      "learning_rate": 0.00017725723275937417,
      "loss": 1.6493,
      "step": 28100
    },
    {
      "epoch": 1.2324097543920987,
      "grad_norm": 4.622398376464844,
      "learning_rate": 0.00017682020802377413,
      "loss": 1.6346,
      "step": 28200
    },
    {
      "epoch": 1.236780001748099,
      "grad_norm": 3.0597727298736572,
      "learning_rate": 0.0001763831832881741,
      "loss": 1.529,
      "step": 28300
    },
    {
      "epoch": 1.2411502491040993,
      "grad_norm": 8.175650596618652,
      "learning_rate": 0.00017594615855257408,
      "loss": 1.5631,
      "step": 28400
    },
    {
      "epoch": 1.2455204964600997,
      "grad_norm": 1.683129906654358,
      "learning_rate": 0.00017550913381697404,
      "loss": 1.6693,
      "step": 28500
    },
    {
      "epoch": 1.2498907438161,
      "grad_norm": 2.9033052921295166,
      "learning_rate": 0.000175072109081374,
      "loss": 1.6203,
      "step": 28600
    },
    {
      "epoch": 1.2542609911721003,
      "grad_norm": 2.8604061603546143,
      "learning_rate": 0.00017463945459312996,
      "loss": 1.6537,
      "step": 28700
    },
    {
      "epoch": 1.2586312385281007,
      "grad_norm": 4.205547332763672,
      "learning_rate": 0.00017420242985752993,
      "loss": 1.5579,
      "step": 28800
    },
    {
      "epoch": 1.263001485884101,
      "grad_norm": 4.785635948181152,
      "learning_rate": 0.00017376540512192991,
      "loss": 1.5509,
      "step": 28900
    },
    {
      "epoch": 1.2673717332401013,
      "grad_norm": 2.860630750656128,
      "learning_rate": 0.00017332838038632985,
      "loss": 1.5812,
      "step": 29000
    },
    {
      "epoch": 1.2717419805961017,
      "grad_norm": 2.176928997039795,
      "learning_rate": 0.0001728913556507298,
      "loss": 1.5942,
      "step": 29100
    },
    {
      "epoch": 1.276112227952102,
      "grad_norm": 3.3119633197784424,
      "learning_rate": 0.00017245433091512977,
      "loss": 1.6411,
      "step": 29200
    },
    {
      "epoch": 1.2804824753081023,
      "grad_norm": 1.653235673904419,
      "learning_rate": 0.00017201730617952973,
      "loss": 1.6044,
      "step": 29300
    },
    {
      "epoch": 1.2848527226641027,
      "grad_norm": 2.81754994392395,
      "learning_rate": 0.0001715802814439297,
      "loss": 1.6023,
      "step": 29400
    },
    {
      "epoch": 1.2892229700201032,
      "grad_norm": 4.419286251068115,
      "learning_rate": 0.00017114325670832968,
      "loss": 1.6726,
      "step": 29500
    },
    {
      "epoch": 1.2935932173761036,
      "grad_norm": 3.2183523178100586,
      "learning_rate": 0.00017070623197272964,
      "loss": 1.6549,
      "step": 29600
    },
    {
      "epoch": 1.2979634647321039,
      "grad_norm": 4.734992504119873,
      "learning_rate": 0.0001702692072371296,
      "loss": 1.6054,
      "step": 29700
    },
    {
      "epoch": 1.3023337120881042,
      "grad_norm": 7.417963981628418,
      "learning_rate": 0.00016983218250152956,
      "loss": 1.6466,
      "step": 29800
    },
    {
      "epoch": 1.3067039594441046,
      "grad_norm": 1.496151089668274,
      "learning_rate": 0.00016939515776592953,
      "loss": 1.5677,
      "step": 29900
    },
    {
      "epoch": 1.311074206800105,
      "grad_norm": 3.917081356048584,
      "learning_rate": 0.00016895813303032951,
      "loss": 1.6472,
      "step": 30000
    },
    {
      "epoch": 1.3154444541561052,
      "grad_norm": 2.8959896564483643,
      "learning_rate": 0.00016852110829472947,
      "loss": 1.5714,
      "step": 30100
    },
    {
      "epoch": 1.3198147015121056,
      "grad_norm": 5.380947589874268,
      "learning_rate": 0.00016808408355912944,
      "loss": 1.5507,
      "step": 30200
    },
    {
      "epoch": 1.324184948868106,
      "grad_norm": 2.905113935470581,
      "learning_rate": 0.0001676470588235294,
      "loss": 1.5709,
      "step": 30300
    },
    {
      "epoch": 1.3285551962241062,
      "grad_norm": 4.27134895324707,
      "learning_rate": 0.00016721003408792936,
      "loss": 1.6012,
      "step": 30400
    },
    {
      "epoch": 1.3329254435801066,
      "grad_norm": 4.382329940795898,
      "learning_rate": 0.00016677300935232935,
      "loss": 1.6217,
      "step": 30500
    },
    {
      "epoch": 1.3372956909361071,
      "grad_norm": 3.1381537914276123,
      "learning_rate": 0.0001663359846167293,
      "loss": 1.5968,
      "step": 30600
    },
    {
      "epoch": 1.3416659382921075,
      "grad_norm": 3.698777198791504,
      "learning_rate": 0.00016589895988112927,
      "loss": 1.5641,
      "step": 30700
    },
    {
      "epoch": 1.3460361856481078,
      "grad_norm": 2.59609055519104,
      "learning_rate": 0.00016546193514552923,
      "loss": 1.5418,
      "step": 30800
    },
    {
      "epoch": 1.3504064330041081,
      "grad_norm": 6.98873233795166,
      "learning_rate": 0.0001650249104099292,
      "loss": 1.5596,
      "step": 30900
    },
    {
      "epoch": 1.3547766803601085,
      "grad_norm": 4.327741622924805,
      "learning_rate": 0.00016458788567432918,
      "loss": 1.6527,
      "step": 31000
    },
    {
      "epoch": 1.3591469277161088,
      "grad_norm": 2.7944934368133545,
      "learning_rate": 0.00016415086093872914,
      "loss": 1.5892,
      "step": 31100
    },
    {
      "epoch": 1.3635171750721091,
      "grad_norm": 3.0989155769348145,
      "learning_rate": 0.00016371383620312907,
      "loss": 1.6906,
      "step": 31200
    },
    {
      "epoch": 1.3678874224281095,
      "grad_norm": 2.2440671920776367,
      "learning_rate": 0.00016327681146752904,
      "loss": 1.5356,
      "step": 31300
    },
    {
      "epoch": 1.3722576697841098,
      "grad_norm": 2.0263679027557373,
      "learning_rate": 0.000162839786731929,
      "loss": 1.5311,
      "step": 31400
    },
    {
      "epoch": 1.3766279171401101,
      "grad_norm": 3.903597116470337,
      "learning_rate": 0.00016240276199632896,
      "loss": 1.5833,
      "step": 31500
    },
    {
      "epoch": 1.3809981644961105,
      "grad_norm": 3.266180992126465,
      "learning_rate": 0.00016196573726072895,
      "loss": 1.6677,
      "step": 31600
    },
    {
      "epoch": 1.3853684118521108,
      "grad_norm": 2.499986410140991,
      "learning_rate": 0.0001615287125251289,
      "loss": 1.6131,
      "step": 31700
    },
    {
      "epoch": 1.3897386592081111,
      "grad_norm": 2.431107759475708,
      "learning_rate": 0.00016109168778952887,
      "loss": 1.5635,
      "step": 31800
    },
    {
      "epoch": 1.3941089065641115,
      "grad_norm": 5.41409158706665,
      "learning_rate": 0.00016065466305392883,
      "loss": 1.6013,
      "step": 31900
    },
    {
      "epoch": 1.3984791539201118,
      "grad_norm": 3.2436373233795166,
      "learning_rate": 0.0001602176383183288,
      "loss": 1.565,
      "step": 32000
    },
    {
      "epoch": 1.4028494012761121,
      "grad_norm": 2.0873312950134277,
      "learning_rate": 0.00015978061358272878,
      "loss": 1.5474,
      "step": 32100
    },
    {
      "epoch": 1.4072196486321125,
      "grad_norm": 2.447134017944336,
      "learning_rate": 0.00015934358884712874,
      "loss": 1.5723,
      "step": 32200
    },
    {
      "epoch": 1.4115898959881128,
      "grad_norm": 3.524728775024414,
      "learning_rate": 0.0001589065641115287,
      "loss": 1.6221,
      "step": 32300
    },
    {
      "epoch": 1.4159601433441134,
      "grad_norm": 2.939223527908325,
      "learning_rate": 0.00015846953937592866,
      "loss": 1.5197,
      "step": 32400
    },
    {
      "epoch": 1.4203303907001137,
      "grad_norm": 4.0600266456604,
      "learning_rate": 0.00015803251464032862,
      "loss": 1.5627,
      "step": 32500
    },
    {
      "epoch": 1.424700638056114,
      "grad_norm": 4.411483287811279,
      "learning_rate": 0.0001575954899047286,
      "loss": 1.559,
      "step": 32600
    },
    {
      "epoch": 1.4290708854121144,
      "grad_norm": 1.9429295063018799,
      "learning_rate": 0.00015715846516912857,
      "loss": 1.5382,
      "step": 32700
    },
    {
      "epoch": 1.4334411327681147,
      "grad_norm": 3.997424840927124,
      "learning_rate": 0.00015672581068088454,
      "loss": 1.56,
      "step": 32800
    },
    {
      "epoch": 1.437811380124115,
      "grad_norm": 1.4134970903396606,
      "learning_rate": 0.0001562887859452845,
      "loss": 1.6167,
      "step": 32900
    },
    {
      "epoch": 1.4421816274801154,
      "grad_norm": 2.638568878173828,
      "learning_rate": 0.00015585176120968446,
      "loss": 1.6233,
      "step": 33000
    },
    {
      "epoch": 1.4465518748361157,
      "grad_norm": 2.307734489440918,
      "learning_rate": 0.00015541473647408442,
      "loss": 1.5838,
      "step": 33100
    },
    {
      "epoch": 1.450922122192116,
      "grad_norm": 2.3023669719696045,
      "learning_rate": 0.0001549777117384844,
      "loss": 1.5441,
      "step": 33200
    },
    {
      "epoch": 1.4552923695481164,
      "grad_norm": 1.8971583843231201,
      "learning_rate": 0.00015454068700288434,
      "loss": 1.6326,
      "step": 33300
    },
    {
      "epoch": 1.4596626169041167,
      "grad_norm": 4.93882942199707,
      "learning_rate": 0.0001541036622672843,
      "loss": 1.5703,
      "step": 33400
    },
    {
      "epoch": 1.4640328642601173,
      "grad_norm": 3.6198573112487793,
      "learning_rate": 0.00015366663753168427,
      "loss": 1.6256,
      "step": 33500
    },
    {
      "epoch": 1.4684031116161176,
      "grad_norm": 3.10078501701355,
      "learning_rate": 0.00015322961279608423,
      "loss": 1.643,
      "step": 33600
    },
    {
      "epoch": 1.472773358972118,
      "grad_norm": 3.7868189811706543,
      "learning_rate": 0.0001527925880604842,
      "loss": 1.5653,
      "step": 33700
    },
    {
      "epoch": 1.4771436063281183,
      "grad_norm": 3.3461103439331055,
      "learning_rate": 0.00015235556332488418,
      "loss": 1.6508,
      "step": 33800
    },
    {
      "epoch": 1.4815138536841186,
      "grad_norm": 3.8147244453430176,
      "learning_rate": 0.00015191853858928414,
      "loss": 1.571,
      "step": 33900
    },
    {
      "epoch": 1.485884101040119,
      "grad_norm": 4.118785858154297,
      "learning_rate": 0.0001514815138536841,
      "loss": 1.6169,
      "step": 34000
    },
    {
      "epoch": 1.4902543483961193,
      "grad_norm": 2.6616742610931396,
      "learning_rate": 0.00015104448911808406,
      "loss": 1.6036,
      "step": 34100
    },
    {
      "epoch": 1.4946245957521196,
      "grad_norm": 4.325055122375488,
      "learning_rate": 0.00015060746438248402,
      "loss": 1.5842,
      "step": 34200
    },
    {
      "epoch": 1.49899484310812,
      "grad_norm": 3.6513993740081787,
      "learning_rate": 0.000150170439646884,
      "loss": 1.5452,
      "step": 34300
    },
    {
      "epoch": 1.5033650904641203,
      "grad_norm": 3.5977072715759277,
      "learning_rate": 0.00014973341491128397,
      "loss": 1.5959,
      "step": 34400
    },
    {
      "epoch": 1.5077353378201206,
      "grad_norm": 3.7399582862854004,
      "learning_rate": 0.00014929639017568393,
      "loss": 1.6316,
      "step": 34500
    },
    {
      "epoch": 1.512105585176121,
      "grad_norm": 5.957705974578857,
      "learning_rate": 0.0001488593654400839,
      "loss": 1.7157,
      "step": 34600
    },
    {
      "epoch": 1.5164758325321213,
      "grad_norm": 3.139782428741455,
      "learning_rate": 0.00014842234070448385,
      "loss": 1.6341,
      "step": 34700
    },
    {
      "epoch": 1.5208460798881216,
      "grad_norm": 3.220996618270874,
      "learning_rate": 0.00014798531596888384,
      "loss": 1.5637,
      "step": 34800
    },
    {
      "epoch": 1.525216327244122,
      "grad_norm": 2.954449415206909,
      "learning_rate": 0.00014754829123328378,
      "loss": 1.5651,
      "step": 34900
    },
    {
      "epoch": 1.5295865746001223,
      "grad_norm": 3.8847732543945312,
      "learning_rate": 0.00014711126649768374,
      "loss": 1.5604,
      "step": 35000
    },
    {
      "epoch": 1.5339568219561226,
      "grad_norm": 3.1394731998443604,
      "learning_rate": 0.00014667861200943973,
      "loss": 1.638,
      "step": 35100
    },
    {
      "epoch": 1.538327069312123,
      "grad_norm": 5.196229934692383,
      "learning_rate": 0.0001462415872738397,
      "loss": 1.4942,
      "step": 35200
    },
    {
      "epoch": 1.5426973166681233,
      "grad_norm": 3.9443819522857666,
      "learning_rate": 0.00014580456253823965,
      "loss": 1.5246,
      "step": 35300
    },
    {
      "epoch": 1.5470675640241238,
      "grad_norm": 5.4565629959106445,
      "learning_rate": 0.0001453675378026396,
      "loss": 1.6056,
      "step": 35400
    },
    {
      "epoch": 1.5514378113801242,
      "grad_norm": 3.4554476737976074,
      "learning_rate": 0.00014493051306703957,
      "loss": 1.6004,
      "step": 35500
    },
    {
      "epoch": 1.5558080587361245,
      "grad_norm": 2.301429271697998,
      "learning_rate": 0.00014449348833143956,
      "loss": 1.5885,
      "step": 35600
    },
    {
      "epoch": 1.5601783060921248,
      "grad_norm": 3.8833422660827637,
      "learning_rate": 0.00014405646359583952,
      "loss": 1.6382,
      "step": 35700
    },
    {
      "epoch": 1.5645485534481252,
      "grad_norm": 5.985939025878906,
      "learning_rate": 0.00014361943886023948,
      "loss": 1.6296,
      "step": 35800
    },
    {
      "epoch": 1.5689188008041255,
      "grad_norm": 2.4421207904815674,
      "learning_rate": 0.00014318241412463945,
      "loss": 1.5981,
      "step": 35900
    },
    {
      "epoch": 1.5732890481601258,
      "grad_norm": 3.474055528640747,
      "learning_rate": 0.0001427453893890394,
      "loss": 1.6061,
      "step": 36000
    },
    {
      "epoch": 1.5776592955161262,
      "grad_norm": 6.423886775970459,
      "learning_rate": 0.00014230836465343937,
      "loss": 1.5807,
      "step": 36100
    },
    {
      "epoch": 1.5820295428721267,
      "grad_norm": 3.6109845638275146,
      "learning_rate": 0.00014187133991783933,
      "loss": 1.618,
      "step": 36200
    },
    {
      "epoch": 1.586399790228127,
      "grad_norm": 5.643677234649658,
      "learning_rate": 0.0001414386854295953,
      "loss": 1.5575,
      "step": 36300
    },
    {
      "epoch": 1.5907700375841274,
      "grad_norm": 1.4728766679763794,
      "learning_rate": 0.00014100166069399526,
      "loss": 1.4789,
      "step": 36400
    },
    {
      "epoch": 1.5951402849401277,
      "grad_norm": 6.509377479553223,
      "learning_rate": 0.00014056463595839524,
      "loss": 1.5591,
      "step": 36500
    },
    {
      "epoch": 1.599510532296128,
      "grad_norm": 1.9117285013198853,
      "learning_rate": 0.0001401276112227952,
      "loss": 1.546,
      "step": 36600
    },
    {
      "epoch": 1.6038807796521284,
      "grad_norm": 2.7480618953704834,
      "learning_rate": 0.00013969058648719517,
      "loss": 1.5363,
      "step": 36700
    },
    {
      "epoch": 1.6082510270081287,
      "grad_norm": 5.085175037384033,
      "learning_rate": 0.00013925356175159513,
      "loss": 1.4833,
      "step": 36800
    },
    {
      "epoch": 1.612621274364129,
      "grad_norm": 4.461696147918701,
      "learning_rate": 0.0001388165370159951,
      "loss": 1.5972,
      "step": 36900
    },
    {
      "epoch": 1.6169915217201294,
      "grad_norm": 3.1731789112091064,
      "learning_rate": 0.00013837951228039508,
      "loss": 1.633,
      "step": 37000
    },
    {
      "epoch": 1.6213617690761297,
      "grad_norm": 5.7260870933532715,
      "learning_rate": 0.000137942487544795,
      "loss": 1.5384,
      "step": 37100
    },
    {
      "epoch": 1.62573201643213,
      "grad_norm": 4.929697513580322,
      "learning_rate": 0.00013750546280919497,
      "loss": 1.6191,
      "step": 37200
    },
    {
      "epoch": 1.6301022637881304,
      "grad_norm": 4.237634181976318,
      "learning_rate": 0.00013706843807359496,
      "loss": 1.5545,
      "step": 37300
    },
    {
      "epoch": 1.6344725111441307,
      "grad_norm": 1.8691023588180542,
      "learning_rate": 0.00013663141333799492,
      "loss": 1.511,
      "step": 37400
    },
    {
      "epoch": 1.638842758500131,
      "grad_norm": 4.386683464050293,
      "learning_rate": 0.00013619438860239488,
      "loss": 1.5084,
      "step": 37500
    },
    {
      "epoch": 1.6432130058561314,
      "grad_norm": 2.8965165615081787,
      "learning_rate": 0.00013575736386679484,
      "loss": 1.6152,
      "step": 37600
    },
    {
      "epoch": 1.6475832532121317,
      "grad_norm": 1.186004400253296,
      "learning_rate": 0.0001353203391311948,
      "loss": 1.5783,
      "step": 37700
    },
    {
      "epoch": 1.651953500568132,
      "grad_norm": 1.2793179750442505,
      "learning_rate": 0.0001348833143955948,
      "loss": 1.4925,
      "step": 37800
    },
    {
      "epoch": 1.6563237479241324,
      "grad_norm": 3.4772045612335205,
      "learning_rate": 0.00013444628965999475,
      "loss": 1.5207,
      "step": 37900
    },
    {
      "epoch": 1.6606939952801327,
      "grad_norm": 4.847428798675537,
      "learning_rate": 0.00013400926492439472,
      "loss": 1.5689,
      "step": 38000
    },
    {
      "epoch": 1.665064242636133,
      "grad_norm": 2.2168002128601074,
      "learning_rate": 0.00013357224018879468,
      "loss": 1.571,
      "step": 38100
    },
    {
      "epoch": 1.6694344899921334,
      "grad_norm": 2.018521547317505,
      "learning_rate": 0.00013313521545319464,
      "loss": 1.5195,
      "step": 38200
    },
    {
      "epoch": 1.673804737348134,
      "grad_norm": 3.900710344314575,
      "learning_rate": 0.0001326981907175946,
      "loss": 1.5494,
      "step": 38300
    },
    {
      "epoch": 1.6781749847041343,
      "grad_norm": 4.3243608474731445,
      "learning_rate": 0.00013226116598199456,
      "loss": 1.5833,
      "step": 38400
    },
    {
      "epoch": 1.6825452320601346,
      "grad_norm": 1.6679894924163818,
      "learning_rate": 0.00013182414124639452,
      "loss": 1.5757,
      "step": 38500
    },
    {
      "epoch": 1.686915479416135,
      "grad_norm": 2.3725361824035645,
      "learning_rate": 0.0001313871165107945,
      "loss": 1.5271,
      "step": 38600
    },
    {
      "epoch": 1.6912857267721353,
      "grad_norm": 4.089207649230957,
      "learning_rate": 0.00013095009177519447,
      "loss": 1.6008,
      "step": 38700
    },
    {
      "epoch": 1.6956559741281356,
      "grad_norm": 2.582777738571167,
      "learning_rate": 0.00013051306703959443,
      "loss": 1.5055,
      "step": 38800
    },
    {
      "epoch": 1.700026221484136,
      "grad_norm": 3.608689546585083,
      "learning_rate": 0.0001300760423039944,
      "loss": 1.5139,
      "step": 38900
    },
    {
      "epoch": 1.7043964688401365,
      "grad_norm": 3.959261417388916,
      "learning_rate": 0.00012963901756839435,
      "loss": 1.5441,
      "step": 39000
    },
    {
      "epoch": 1.7087667161961368,
      "grad_norm": 3.297415018081665,
      "learning_rate": 0.00012920199283279434,
      "loss": 1.5427,
      "step": 39100
    },
    {
      "epoch": 1.7131369635521372,
      "grad_norm": 5.345671653747559,
      "learning_rate": 0.00012876496809719428,
      "loss": 1.5936,
      "step": 39200
    },
    {
      "epoch": 1.7175072109081375,
      "grad_norm": 3.630502223968506,
      "learning_rate": 0.00012832794336159424,
      "loss": 1.5153,
      "step": 39300
    },
    {
      "epoch": 1.7218774582641379,
      "grad_norm": 2.553212881088257,
      "learning_rate": 0.00012789091862599423,
      "loss": 1.619,
      "step": 39400
    },
    {
      "epoch": 1.7262477056201382,
      "grad_norm": 4.626340866088867,
      "learning_rate": 0.0001274538938903942,
      "loss": 1.5232,
      "step": 39500
    },
    {
      "epoch": 1.7306179529761385,
      "grad_norm": 2.5038328170776367,
      "learning_rate": 0.00012701686915479415,
      "loss": 1.5867,
      "step": 39600
    },
    {
      "epoch": 1.7349882003321389,
      "grad_norm": 2.9676783084869385,
      "learning_rate": 0.0001265798444191941,
      "loss": 1.547,
      "step": 39700
    },
    {
      "epoch": 1.7393584476881392,
      "grad_norm": 3.8634865283966064,
      "learning_rate": 0.00012614281968359407,
      "loss": 1.5812,
      "step": 39800
    },
    {
      "epoch": 1.7437286950441395,
      "grad_norm": 3.300320625305176,
      "learning_rate": 0.00012570579494799406,
      "loss": 1.6302,
      "step": 39900
    },
    {
      "epoch": 1.7480989424001399,
      "grad_norm": 3.573540449142456,
      "learning_rate": 0.00012526877021239402,
      "loss": 1.6252,
      "step": 40000
    },
    {
      "epoch": 1.7524691897561402,
      "grad_norm": 2.4092509746551514,
      "learning_rate": 0.00012483174547679398,
      "loss": 1.5629,
      "step": 40100
    },
    {
      "epoch": 1.7568394371121405,
      "grad_norm": 3.676525115966797,
      "learning_rate": 0.00012439472074119394,
      "loss": 1.4949,
      "step": 40200
    },
    {
      "epoch": 1.7612096844681409,
      "grad_norm": 3.955902576446533,
      "learning_rate": 0.0001239576960055939,
      "loss": 1.606,
      "step": 40300
    },
    {
      "epoch": 1.7655799318241412,
      "grad_norm": 3.0686662197113037,
      "learning_rate": 0.00012352067126999386,
      "loss": 1.6429,
      "step": 40400
    },
    {
      "epoch": 1.7699501791801415,
      "grad_norm": 2.5390419960021973,
      "learning_rate": 0.00012308364653439383,
      "loss": 1.477,
      "step": 40500
    },
    {
      "epoch": 1.7743204265361419,
      "grad_norm": 2.027639865875244,
      "learning_rate": 0.0001226466217987938,
      "loss": 1.5416,
      "step": 40600
    },
    {
      "epoch": 1.7786906738921422,
      "grad_norm": 4.451761245727539,
      "learning_rate": 0.00012220959706319378,
      "loss": 1.6028,
      "step": 40700
    },
    {
      "epoch": 1.7830609212481425,
      "grad_norm": 3.448920726776123,
      "learning_rate": 0.00012177257232759374,
      "loss": 1.6298,
      "step": 40800
    },
    {
      "epoch": 1.7874311686041429,
      "grad_norm": 2.3817152976989746,
      "learning_rate": 0.0001213355475919937,
      "loss": 1.5283,
      "step": 40900
    },
    {
      "epoch": 1.7918014159601432,
      "grad_norm": 2.149867057800293,
      "learning_rate": 0.00012089852285639366,
      "loss": 1.5895,
      "step": 41000
    },
    {
      "epoch": 1.7961716633161435,
      "grad_norm": 5.783982276916504,
      "learning_rate": 0.00012046149812079363,
      "loss": 1.5267,
      "step": 41100
    },
    {
      "epoch": 1.800541910672144,
      "grad_norm": 3.2002599239349365,
      "learning_rate": 0.0001200244733851936,
      "loss": 1.5618,
      "step": 41200
    },
    {
      "epoch": 1.8049121580281444,
      "grad_norm": 4.140152931213379,
      "learning_rate": 0.00011958744864959357,
      "loss": 1.5437,
      "step": 41300
    },
    {
      "epoch": 1.8092824053841448,
      "grad_norm": 3.274047613143921,
      "learning_rate": 0.00011915042391399352,
      "loss": 1.5887,
      "step": 41400
    },
    {
      "epoch": 1.813652652740145,
      "grad_norm": 5.215096473693848,
      "learning_rate": 0.00011871339917839348,
      "loss": 1.6019,
      "step": 41500
    },
    {
      "epoch": 1.8180229000961454,
      "grad_norm": 4.279750347137451,
      "learning_rate": 0.00011827637444279345,
      "loss": 1.5236,
      "step": 41600
    },
    {
      "epoch": 1.8223931474521458,
      "grad_norm": 2.8591361045837402,
      "learning_rate": 0.00011784371995454942,
      "loss": 1.6119,
      "step": 41700
    },
    {
      "epoch": 1.826763394808146,
      "grad_norm": 3.5864827632904053,
      "learning_rate": 0.00011740669521894938,
      "loss": 1.5942,
      "step": 41800
    },
    {
      "epoch": 1.8311336421641466,
      "grad_norm": 3.7149038314819336,
      "learning_rate": 0.00011696967048334935,
      "loss": 1.5894,
      "step": 41900
    },
    {
      "epoch": 1.835503889520147,
      "grad_norm": 3.390106678009033,
      "learning_rate": 0.00011653264574774931,
      "loss": 1.6466,
      "step": 42000
    },
    {
      "epoch": 1.8398741368761473,
      "grad_norm": 5.6471967697143555,
      "learning_rate": 0.00011609562101214928,
      "loss": 1.4638,
      "step": 42100
    },
    {
      "epoch": 1.8442443842321476,
      "grad_norm": 2.9861013889312744,
      "learning_rate": 0.00011565859627654925,
      "loss": 1.6183,
      "step": 42200
    },
    {
      "epoch": 1.848614631588148,
      "grad_norm": 2.806596040725708,
      "learning_rate": 0.00011522157154094921,
      "loss": 1.5895,
      "step": 42300
    },
    {
      "epoch": 1.8529848789441483,
      "grad_norm": 3.9366867542266846,
      "learning_rate": 0.00011478454680534916,
      "loss": 1.5352,
      "step": 42400
    },
    {
      "epoch": 1.8573551263001487,
      "grad_norm": 2.809659957885742,
      "learning_rate": 0.00011434752206974913,
      "loss": 1.5061,
      "step": 42500
    },
    {
      "epoch": 1.861725373656149,
      "grad_norm": 3.7784268856048584,
      "learning_rate": 0.0001139104973341491,
      "loss": 1.5119,
      "step": 42600
    },
    {
      "epoch": 1.8660956210121493,
      "grad_norm": 5.702380657196045,
      "learning_rate": 0.00011347347259854907,
      "loss": 1.5214,
      "step": 42700
    },
    {
      "epoch": 1.8704658683681497,
      "grad_norm": 4.552339553833008,
      "learning_rate": 0.00011303644786294903,
      "loss": 1.5719,
      "step": 42800
    },
    {
      "epoch": 1.87483611572415,
      "grad_norm": 3.880319833755493,
      "learning_rate": 0.00011259942312734899,
      "loss": 1.5026,
      "step": 42900
    },
    {
      "epoch": 1.8792063630801503,
      "grad_norm": 2.84450101852417,
      "learning_rate": 0.00011216239839174897,
      "loss": 1.6036,
      "step": 43000
    },
    {
      "epoch": 1.8835766104361507,
      "grad_norm": 2.4882872104644775,
      "learning_rate": 0.00011172537365614893,
      "loss": 1.5104,
      "step": 43100
    },
    {
      "epoch": 1.887946857792151,
      "grad_norm": 4.2641754150390625,
      "learning_rate": 0.0001112883489205489,
      "loss": 1.5226,
      "step": 43200
    },
    {
      "epoch": 1.8923171051481513,
      "grad_norm": 1.6977320909500122,
      "learning_rate": 0.00011085132418494886,
      "loss": 1.521,
      "step": 43300
    },
    {
      "epoch": 1.8966873525041517,
      "grad_norm": 2.5848443508148193,
      "learning_rate": 0.00011041429944934883,
      "loss": 1.4901,
      "step": 43400
    },
    {
      "epoch": 1.901057599860152,
      "grad_norm": 5.233823776245117,
      "learning_rate": 0.00010997727471374879,
      "loss": 1.5394,
      "step": 43500
    },
    {
      "epoch": 1.9054278472161523,
      "grad_norm": 4.543996334075928,
      "learning_rate": 0.00010954024997814875,
      "loss": 1.4833,
      "step": 43600
    },
    {
      "epoch": 1.9097980945721527,
      "grad_norm": 2.5091075897216797,
      "learning_rate": 0.00010910322524254871,
      "loss": 1.5432,
      "step": 43700
    },
    {
      "epoch": 1.914168341928153,
      "grad_norm": 2.2507097721099854,
      "learning_rate": 0.00010866620050694868,
      "loss": 1.5968,
      "step": 43800
    },
    {
      "epoch": 1.9185385892841533,
      "grad_norm": 3.808897018432617,
      "learning_rate": 0.00010822917577134864,
      "loss": 1.6232,
      "step": 43900
    },
    {
      "epoch": 1.9229088366401539,
      "grad_norm": 4.845458984375,
      "learning_rate": 0.00010779215103574862,
      "loss": 1.6522,
      "step": 44000
    },
    {
      "epoch": 1.9272790839961542,
      "grad_norm": 2.6208348274230957,
      "learning_rate": 0.00010735512630014858,
      "loss": 1.586,
      "step": 44100
    },
    {
      "epoch": 1.9316493313521546,
      "grad_norm": 2.6377038955688477,
      "learning_rate": 0.00010691810156454854,
      "loss": 1.5256,
      "step": 44200
    },
    {
      "epoch": 1.9360195787081549,
      "grad_norm": 3.669645309448242,
      "learning_rate": 0.00010648107682894852,
      "loss": 1.445,
      "step": 44300
    },
    {
      "epoch": 1.9403898260641552,
      "grad_norm": 2.4751269817352295,
      "learning_rate": 0.00010604405209334848,
      "loss": 1.5808,
      "step": 44400
    },
    {
      "epoch": 1.9447600734201556,
      "grad_norm": 5.442782878875732,
      "learning_rate": 0.00010560702735774845,
      "loss": 1.5345,
      "step": 44500
    },
    {
      "epoch": 1.949130320776156,
      "grad_norm": 2.3456623554229736,
      "learning_rate": 0.0001051700026221484,
      "loss": 1.5742,
      "step": 44600
    },
    {
      "epoch": 1.9535005681321564,
      "grad_norm": 1.5662413835525513,
      "learning_rate": 0.00010473297788654836,
      "loss": 1.5011,
      "step": 44700
    },
    {
      "epoch": 1.9578708154881568,
      "grad_norm": 3.099348783493042,
      "learning_rate": 0.00010429595315094834,
      "loss": 1.5147,
      "step": 44800
    },
    {
      "epoch": 1.9622410628441571,
      "grad_norm": 2.1262285709381104,
      "learning_rate": 0.0001038589284153483,
      "loss": 1.5381,
      "step": 44900
    },
    {
      "epoch": 1.9666113102001574,
      "grad_norm": 3.4666740894317627,
      "learning_rate": 0.00010342627392710426,
      "loss": 1.5652,
      "step": 45000
    },
    {
      "epoch": 1.9709815575561578,
      "grad_norm": 3.885077714920044,
      "learning_rate": 0.00010298924919150424,
      "loss": 1.563,
      "step": 45100
    },
    {
      "epoch": 1.9753518049121581,
      "grad_norm": 3.6094090938568115,
      "learning_rate": 0.0001025522244559042,
      "loss": 1.5173,
      "step": 45200
    },
    {
      "epoch": 1.9797220522681584,
      "grad_norm": 3.315687656402588,
      "learning_rate": 0.00010211519972030416,
      "loss": 1.5072,
      "step": 45300
    },
    {
      "epoch": 1.9840922996241588,
      "grad_norm": 4.5512919425964355,
      "learning_rate": 0.00010167817498470413,
      "loss": 1.4942,
      "step": 45400
    },
    {
      "epoch": 1.9884625469801591,
      "grad_norm": 1.67251455783844,
      "learning_rate": 0.0001012411502491041,
      "loss": 1.4948,
      "step": 45500
    },
    {
      "epoch": 1.9928327943361595,
      "grad_norm": 4.203737258911133,
      "learning_rate": 0.00010080412551350407,
      "loss": 1.552,
      "step": 45600
    },
    {
      "epoch": 1.9972030416921598,
      "grad_norm": 2.9544386863708496,
      "learning_rate": 0.00010036710077790402,
      "loss": 1.5346,
      "step": 45700
    },
    {
      "epoch": 2.00157328904816,
      "grad_norm": 2.212724447250366,
      "learning_rate": 9.993007604230398e-05,
      "loss": 1.4677,
      "step": 45800
    },
    {
      "epoch": 2.0059435364041605,
      "grad_norm": 4.348299503326416,
      "learning_rate": 9.949305130670395e-05,
      "loss": 1.4817,
      "step": 45900
    },
    {
      "epoch": 2.010313783760161,
      "grad_norm": 3.0465028285980225,
      "learning_rate": 9.905602657110391e-05,
      "loss": 1.556,
      "step": 46000
    },
    {
      "epoch": 2.014684031116161,
      "grad_norm": 2.227116346359253,
      "learning_rate": 9.861900183550388e-05,
      "loss": 1.5028,
      "step": 46100
    },
    {
      "epoch": 2.0190542784721615,
      "grad_norm": 4.574718952178955,
      "learning_rate": 9.818197709990385e-05,
      "loss": 1.4427,
      "step": 46200
    },
    {
      "epoch": 2.023424525828162,
      "grad_norm": 5.056845664978027,
      "learning_rate": 9.774495236430381e-05,
      "loss": 1.5194,
      "step": 46300
    },
    {
      "epoch": 2.027794773184162,
      "grad_norm": 4.140745162963867,
      "learning_rate": 9.730792762870379e-05,
      "loss": 1.5421,
      "step": 46400
    },
    {
      "epoch": 2.0321650205401625,
      "grad_norm": 3.559769630432129,
      "learning_rate": 9.687090289310375e-05,
      "loss": 1.4468,
      "step": 46500
    },
    {
      "epoch": 2.036535267896163,
      "grad_norm": 2.935551166534424,
      "learning_rate": 9.643387815750371e-05,
      "loss": 1.5899,
      "step": 46600
    },
    {
      "epoch": 2.040905515252163,
      "grad_norm": 4.10396671295166,
      "learning_rate": 9.599685342190367e-05,
      "loss": 1.507,
      "step": 46700
    },
    {
      "epoch": 2.0452757626081635,
      "grad_norm": 4.540011882781982,
      "learning_rate": 9.555982868630363e-05,
      "loss": 1.4833,
      "step": 46800
    },
    {
      "epoch": 2.049646009964164,
      "grad_norm": 1.9069629907608032,
      "learning_rate": 9.512280395070359e-05,
      "loss": 1.4645,
      "step": 46900
    },
    {
      "epoch": 2.054016257320164,
      "grad_norm": 3.4103376865386963,
      "learning_rate": 9.468577921510357e-05,
      "loss": 1.5327,
      "step": 47000
    },
    {
      "epoch": 2.0583865046761645,
      "grad_norm": 3.787203073501587,
      "learning_rate": 9.424875447950353e-05,
      "loss": 1.5817,
      "step": 47100
    },
    {
      "epoch": 2.062756752032165,
      "grad_norm": 4.221193790435791,
      "learning_rate": 9.38117297439035e-05,
      "loss": 1.4836,
      "step": 47200
    },
    {
      "epoch": 2.0671269993881656,
      "grad_norm": 4.230991840362549,
      "learning_rate": 9.337470500830346e-05,
      "loss": 1.531,
      "step": 47300
    },
    {
      "epoch": 2.071497246744166,
      "grad_norm": 4.305332660675049,
      "learning_rate": 9.293768027270342e-05,
      "loss": 1.5104,
      "step": 47400
    },
    {
      "epoch": 2.0758674941001662,
      "grad_norm": 2.89040207862854,
      "learning_rate": 9.25006555371034e-05,
      "loss": 1.4915,
      "step": 47500
    },
    {
      "epoch": 2.0802377414561666,
      "grad_norm": 3.1340527534484863,
      "learning_rate": 9.206363080150336e-05,
      "loss": 1.5123,
      "step": 47600
    },
    {
      "epoch": 2.084607988812167,
      "grad_norm": 2.1321864128112793,
      "learning_rate": 9.163534656061533e-05,
      "loss": 1.457,
      "step": 47700
    },
    {
      "epoch": 2.0889782361681672,
      "grad_norm": 2.4321703910827637,
      "learning_rate": 9.119832182501528e-05,
      "loss": 1.5219,
      "step": 47800
    },
    {
      "epoch": 2.0933484835241676,
      "grad_norm": 4.434728622436523,
      "learning_rate": 9.076129708941525e-05,
      "loss": 1.4549,
      "step": 47900
    },
    {
      "epoch": 2.097718730880168,
      "grad_norm": 1.747031807899475,
      "learning_rate": 9.032427235381521e-05,
      "loss": 1.503,
      "step": 48000
    },
    {
      "epoch": 2.1020889782361682,
      "grad_norm": 2.63932728767395,
      "learning_rate": 8.988724761821519e-05,
      "loss": 1.4971,
      "step": 48100
    },
    {
      "epoch": 2.1064592255921686,
      "grad_norm": 7.010124206542969,
      "learning_rate": 8.945022288261515e-05,
      "loss": 1.5076,
      "step": 48200
    },
    {
      "epoch": 2.110829472948169,
      "grad_norm": 4.398212909698486,
      "learning_rate": 8.901319814701511e-05,
      "loss": 1.6109,
      "step": 48300
    },
    {
      "epoch": 2.1151997203041693,
      "grad_norm": 4.678847789764404,
      "learning_rate": 8.857617341141508e-05,
      "loss": 1.5577,
      "step": 48400
    },
    {
      "epoch": 2.1195699676601696,
      "grad_norm": 3.1760058403015137,
      "learning_rate": 8.813914867581504e-05,
      "loss": 1.4689,
      "step": 48500
    },
    {
      "epoch": 2.12394021501617,
      "grad_norm": 3.6378767490386963,
      "learning_rate": 8.770212394021502e-05,
      "loss": 1.4956,
      "step": 48600
    },
    {
      "epoch": 2.1283104623721703,
      "grad_norm": 4.277217388153076,
      "learning_rate": 8.726509920461498e-05,
      "loss": 1.5022,
      "step": 48700
    },
    {
      "epoch": 2.1326807097281706,
      "grad_norm": 2.4619789123535156,
      "learning_rate": 8.682807446901494e-05,
      "loss": 1.5105,
      "step": 48800
    },
    {
      "epoch": 2.137050957084171,
      "grad_norm": 5.939774990081787,
      "learning_rate": 8.63910497334149e-05,
      "loss": 1.5492,
      "step": 48900
    },
    {
      "epoch": 2.1414212044401713,
      "grad_norm": 2.2444472312927246,
      "learning_rate": 8.595402499781486e-05,
      "loss": 1.5155,
      "step": 49000
    },
    {
      "epoch": 2.1457914517961716,
      "grad_norm": 3.029665231704712,
      "learning_rate": 8.551700026221483e-05,
      "loss": 1.5059,
      "step": 49100
    },
    {
      "epoch": 2.150161699152172,
      "grad_norm": 3.776334047317505,
      "learning_rate": 8.50799755266148e-05,
      "loss": 1.5691,
      "step": 49200
    },
    {
      "epoch": 2.1545319465081723,
      "grad_norm": 2.3994550704956055,
      "learning_rate": 8.464295079101476e-05,
      "loss": 1.4359,
      "step": 49300
    },
    {
      "epoch": 2.1589021938641726,
      "grad_norm": 4.9731245040893555,
      "learning_rate": 8.420592605541474e-05,
      "loss": 1.4873,
      "step": 49400
    },
    {
      "epoch": 2.163272441220173,
      "grad_norm": 4.480464935302734,
      "learning_rate": 8.37689013198147e-05,
      "loss": 1.459,
      "step": 49500
    },
    {
      "epoch": 2.1676426885761733,
      "grad_norm": 3.667809009552002,
      "learning_rate": 8.333187658421466e-05,
      "loss": 1.5593,
      "step": 49600
    },
    {
      "epoch": 2.1720129359321736,
      "grad_norm": 3.3846678733825684,
      "learning_rate": 8.289485184861463e-05,
      "loss": 1.4303,
      "step": 49700
    },
    {
      "epoch": 2.176383183288174,
      "grad_norm": 2.3532118797302246,
      "learning_rate": 8.24578271130146e-05,
      "loss": 1.4741,
      "step": 49800
    },
    {
      "epoch": 2.1807534306441743,
      "grad_norm": 6.017586708068848,
      "learning_rate": 8.202080237741454e-05,
      "loss": 1.5055,
      "step": 49900
    },
    {
      "epoch": 2.185123678000175,
      "grad_norm": 2.737471103668213,
      "learning_rate": 8.158377764181452e-05,
      "loss": 1.6072,
      "step": 50000
    },
    {
      "epoch": 2.1894939253561754,
      "grad_norm": 1.255576252937317,
      "learning_rate": 8.114675290621448e-05,
      "loss": 1.5219,
      "step": 50100
    },
    {
      "epoch": 2.1938641727121757,
      "grad_norm": 3.894191026687622,
      "learning_rate": 8.070972817061445e-05,
      "loss": 1.4751,
      "step": 50200
    },
    {
      "epoch": 2.198234420068176,
      "grad_norm": 11.420247077941895,
      "learning_rate": 8.027270343501441e-05,
      "loss": 1.5059,
      "step": 50300
    },
    {
      "epoch": 2.2026046674241764,
      "grad_norm": 3.8028557300567627,
      "learning_rate": 7.983567869941437e-05,
      "loss": 1.5116,
      "step": 50400
    },
    {
      "epoch": 2.2069749147801767,
      "grad_norm": 3.607848882675171,
      "learning_rate": 7.939865396381435e-05,
      "loss": 1.5561,
      "step": 50500
    },
    {
      "epoch": 2.211345162136177,
      "grad_norm": 4.949463844299316,
      "learning_rate": 7.896162922821431e-05,
      "loss": 1.4452,
      "step": 50600
    },
    {
      "epoch": 2.2157154094921774,
      "grad_norm": 1.937291145324707,
      "learning_rate": 7.852460449261429e-05,
      "loss": 1.506,
      "step": 50700
    },
    {
      "epoch": 2.2200856568481777,
      "grad_norm": 2.7900354862213135,
      "learning_rate": 7.808757975701425e-05,
      "loss": 1.5383,
      "step": 50800
    },
    {
      "epoch": 2.224455904204178,
      "grad_norm": 3.7620108127593994,
      "learning_rate": 7.765055502141421e-05,
      "loss": 1.4809,
      "step": 50900
    },
    {
      "epoch": 2.2288261515601784,
      "grad_norm": 5.064439296722412,
      "learning_rate": 7.721353028581417e-05,
      "loss": 1.6509,
      "step": 51000
    },
    {
      "epoch": 2.2331963989161787,
      "grad_norm": 2.3972764015197754,
      "learning_rate": 7.677650555021413e-05,
      "loss": 1.497,
      "step": 51100
    },
    {
      "epoch": 2.237566646272179,
      "grad_norm": 1.7966328859329224,
      "learning_rate": 7.633948081461409e-05,
      "loss": 1.5076,
      "step": 51200
    },
    {
      "epoch": 2.2419368936281794,
      "grad_norm": 4.512871265411377,
      "learning_rate": 7.590245607901407e-05,
      "loss": 1.4869,
      "step": 51300
    },
    {
      "epoch": 2.2463071409841797,
      "grad_norm": 4.294355869293213,
      "learning_rate": 7.546543134341403e-05,
      "loss": 1.6235,
      "step": 51400
    },
    {
      "epoch": 2.25067738834018,
      "grad_norm": 1.7362416982650757,
      "learning_rate": 7.5028406607814e-05,
      "loss": 1.5715,
      "step": 51500
    },
    {
      "epoch": 2.2550476356961804,
      "grad_norm": 2.9762158393859863,
      "learning_rate": 7.459138187221396e-05,
      "loss": 1.4173,
      "step": 51600
    },
    {
      "epoch": 2.2594178830521807,
      "grad_norm": 6.626598834991455,
      "learning_rate": 7.415435713661392e-05,
      "loss": 1.5433,
      "step": 51700
    },
    {
      "epoch": 2.263788130408181,
      "grad_norm": 2.7764506340026855,
      "learning_rate": 7.371733240101389e-05,
      "loss": 1.521,
      "step": 51800
    },
    {
      "epoch": 2.2681583777641814,
      "grad_norm": 1.3530229330062866,
      "learning_rate": 7.328030766541386e-05,
      "loss": 1.5038,
      "step": 51900
    },
    {
      "epoch": 2.2725286251201817,
      "grad_norm": 3.024411916732788,
      "learning_rate": 7.284328292981382e-05,
      "loss": 1.463,
      "step": 52000
    },
    {
      "epoch": 2.276898872476182,
      "grad_norm": 6.325848579406738,
      "learning_rate": 7.241062844156979e-05,
      "loss": 1.5192,
      "step": 52100
    },
    {
      "epoch": 2.2812691198321824,
      "grad_norm": 2.822153091430664,
      "learning_rate": 7.197360370596976e-05,
      "loss": 1.48,
      "step": 52200
    },
    {
      "epoch": 2.2856393671881827,
      "grad_norm": 4.5891432762146,
      "learning_rate": 7.153657897036972e-05,
      "loss": 1.4932,
      "step": 52300
    },
    {
      "epoch": 2.290009614544183,
      "grad_norm": 3.4633536338806152,
      "learning_rate": 7.110392448212567e-05,
      "loss": 1.5489,
      "step": 52400
    },
    {
      "epoch": 2.2943798619001834,
      "grad_norm": 3.273026704788208,
      "learning_rate": 7.066689974652565e-05,
      "loss": 1.4946,
      "step": 52500
    },
    {
      "epoch": 2.2987501092561837,
      "grad_norm": 4.220214366912842,
      "learning_rate": 7.022987501092561e-05,
      "loss": 1.5299,
      "step": 52600
    },
    {
      "epoch": 2.303120356612184,
      "grad_norm": 4.5150227546691895,
      "learning_rate": 6.979285027532558e-05,
      "loss": 1.4809,
      "step": 52700
    },
    {
      "epoch": 2.3074906039681844,
      "grad_norm": 2.871448040008545,
      "learning_rate": 6.935582553972554e-05,
      "loss": 1.4366,
      "step": 52800
    },
    {
      "epoch": 2.3118608513241847,
      "grad_norm": 2.8104605674743652,
      "learning_rate": 6.89188008041255e-05,
      "loss": 1.5123,
      "step": 52900
    },
    {
      "epoch": 2.316231098680185,
      "grad_norm": 2.4093663692474365,
      "learning_rate": 6.848177606852547e-05,
      "loss": 1.4605,
      "step": 53000
    },
    {
      "epoch": 2.320601346036186,
      "grad_norm": 4.035274982452393,
      "learning_rate": 6.804475133292544e-05,
      "loss": 1.5019,
      "step": 53100
    },
    {
      "epoch": 2.324971593392186,
      "grad_norm": 3.2723171710968018,
      "learning_rate": 6.76077265973254e-05,
      "loss": 1.4668,
      "step": 53200
    },
    {
      "epoch": 2.3293418407481865,
      "grad_norm": 3.0271341800689697,
      "learning_rate": 6.717070186172538e-05,
      "loss": 1.5747,
      "step": 53300
    },
    {
      "epoch": 2.333712088104187,
      "grad_norm": 2.486205577850342,
      "learning_rate": 6.673367712612532e-05,
      "loss": 1.5161,
      "step": 53400
    },
    {
      "epoch": 2.338082335460187,
      "grad_norm": 4.544338703155518,
      "learning_rate": 6.62966523905253e-05,
      "loss": 1.5249,
      "step": 53500
    },
    {
      "epoch": 2.3424525828161875,
      "grad_norm": 2.291484832763672,
      "learning_rate": 6.585962765492526e-05,
      "loss": 1.4844,
      "step": 53600
    },
    {
      "epoch": 2.346822830172188,
      "grad_norm": 4.247817516326904,
      "learning_rate": 6.542260291932524e-05,
      "loss": 1.4773,
      "step": 53700
    },
    {
      "epoch": 2.351193077528188,
      "grad_norm": 4.391452312469482,
      "learning_rate": 6.49855781837252e-05,
      "loss": 1.487,
      "step": 53800
    },
    {
      "epoch": 2.3555633248841885,
      "grad_norm": 3.5994186401367188,
      "learning_rate": 6.454855344812516e-05,
      "loss": 1.5433,
      "step": 53900
    },
    {
      "epoch": 2.359933572240189,
      "grad_norm": 3.2207281589508057,
      "learning_rate": 6.411152871252512e-05,
      "loss": 1.47,
      "step": 54000
    },
    {
      "epoch": 2.364303819596189,
      "grad_norm": 4.240184307098389,
      "learning_rate": 6.367450397692508e-05,
      "loss": 1.4252,
      "step": 54100
    },
    {
      "epoch": 2.3686740669521895,
      "grad_norm": 2.0682833194732666,
      "learning_rate": 6.323747924132505e-05,
      "loss": 1.5563,
      "step": 54200
    },
    {
      "epoch": 2.37304431430819,
      "grad_norm": 3.477954864501953,
      "learning_rate": 6.280045450572502e-05,
      "loss": 1.5135,
      "step": 54300
    },
    {
      "epoch": 2.37741456166419,
      "grad_norm": 5.87410306930542,
      "learning_rate": 6.236342977012499e-05,
      "loss": 1.4831,
      "step": 54400
    },
    {
      "epoch": 2.3817848090201905,
      "grad_norm": 3.1230785846710205,
      "learning_rate": 6.192640503452494e-05,
      "loss": 1.4958,
      "step": 54500
    },
    {
      "epoch": 2.386155056376191,
      "grad_norm": 2.1022229194641113,
      "learning_rate": 6.148938029892491e-05,
      "loss": 1.4698,
      "step": 54600
    },
    {
      "epoch": 2.390525303732191,
      "grad_norm": 2.6616768836975098,
      "learning_rate": 6.105235556332487e-05,
      "loss": 1.4562,
      "step": 54700
    },
    {
      "epoch": 2.3948955510881915,
      "grad_norm": 4.904973030090332,
      "learning_rate": 6.061533082772484e-05,
      "loss": 1.4705,
      "step": 54800
    },
    {
      "epoch": 2.399265798444192,
      "grad_norm": 3.3159477710723877,
      "learning_rate": 6.017830609212481e-05,
      "loss": 1.4323,
      "step": 54900
    },
    {
      "epoch": 2.403636045800192,
      "grad_norm": 3.8237342834472656,
      "learning_rate": 5.974128135652477e-05,
      "loss": 1.5379,
      "step": 55000
    },
    {
      "epoch": 2.4080062931561925,
      "grad_norm": 3.213456869125366,
      "learning_rate": 5.930425662092474e-05,
      "loss": 1.4915,
      "step": 55100
    },
    {
      "epoch": 2.412376540512193,
      "grad_norm": 1.5870088338851929,
      "learning_rate": 5.88672318853247e-05,
      "loss": 1.4593,
      "step": 55200
    },
    {
      "epoch": 2.416746787868193,
      "grad_norm": 5.644240856170654,
      "learning_rate": 5.843020714972467e-05,
      "loss": 1.4857,
      "step": 55300
    },
    {
      "epoch": 2.4211170352241935,
      "grad_norm": 4.915252685546875,
      "learning_rate": 5.7993182414124636e-05,
      "loss": 1.467,
      "step": 55400
    },
    {
      "epoch": 2.425487282580194,
      "grad_norm": 2.9559268951416016,
      "learning_rate": 5.7556157678524604e-05,
      "loss": 1.4793,
      "step": 55500
    },
    {
      "epoch": 2.4298575299361946,
      "grad_norm": 2.706705093383789,
      "learning_rate": 5.711913294292456e-05,
      "loss": 1.5665,
      "step": 55600
    },
    {
      "epoch": 2.434227777292195,
      "grad_norm": 1.7780364751815796,
      "learning_rate": 5.668647845468053e-05,
      "loss": 1.552,
      "step": 55700
    },
    {
      "epoch": 2.4385980246481953,
      "grad_norm": 4.1002278327941895,
      "learning_rate": 5.62494537190805e-05,
      "loss": 1.478,
      "step": 55800
    },
    {
      "epoch": 2.4429682720041956,
      "grad_norm": 3.9829366207122803,
      "learning_rate": 5.581242898348046e-05,
      "loss": 1.5209,
      "step": 55900
    },
    {
      "epoch": 2.447338519360196,
      "grad_norm": 4.011308670043945,
      "learning_rate": 5.537540424788043e-05,
      "loss": 1.5677,
      "step": 56000
    },
    {
      "epoch": 2.4517087667161963,
      "grad_norm": 3.272423505783081,
      "learning_rate": 5.493837951228039e-05,
      "loss": 1.5111,
      "step": 56100
    },
    {
      "epoch": 2.4560790140721966,
      "grad_norm": 3.041222333908081,
      "learning_rate": 5.4501354776680356e-05,
      "loss": 1.4794,
      "step": 56200
    },
    {
      "epoch": 2.460449261428197,
      "grad_norm": 4.704472541809082,
      "learning_rate": 5.406433004108032e-05,
      "loss": 1.5452,
      "step": 56300
    },
    {
      "epoch": 2.4648195087841973,
      "grad_norm": 1.4997203350067139,
      "learning_rate": 5.3627305305480285e-05,
      "loss": 1.4518,
      "step": 56400
    },
    {
      "epoch": 2.4691897561401976,
      "grad_norm": 2.6368558406829834,
      "learning_rate": 5.319028056988025e-05,
      "loss": 1.4558,
      "step": 56500
    },
    {
      "epoch": 2.473560003496198,
      "grad_norm": 2.234321355819702,
      "learning_rate": 5.275325583428022e-05,
      "loss": 1.5034,
      "step": 56600
    },
    {
      "epoch": 2.4779302508521983,
      "grad_norm": 3.1464757919311523,
      "learning_rate": 5.2316231098680176e-05,
      "loss": 1.5313,
      "step": 56700
    },
    {
      "epoch": 2.4823004982081986,
      "grad_norm": 3.1058895587921143,
      "learning_rate": 5.1879206363080144e-05,
      "loss": 1.4969,
      "step": 56800
    },
    {
      "epoch": 2.486670745564199,
      "grad_norm": 5.136849403381348,
      "learning_rate": 5.144218162748011e-05,
      "loss": 1.5527,
      "step": 56900
    },
    {
      "epoch": 2.4910409929201993,
      "grad_norm": 3.6469039916992188,
      "learning_rate": 5.100515689188008e-05,
      "loss": 1.4227,
      "step": 57000
    },
    {
      "epoch": 2.4954112402761996,
      "grad_norm": 3.5633797645568848,
      "learning_rate": 5.056813215628005e-05,
      "loss": 1.4689,
      "step": 57100
    },
    {
      "epoch": 2.4997814876322,
      "grad_norm": 2.5767197608947754,
      "learning_rate": 5.013110742068e-05,
      "loss": 1.4925,
      "step": 57200
    },
    {
      "epoch": 2.5041517349882003,
      "grad_norm": 3.263458490371704,
      "learning_rate": 4.969408268507997e-05,
      "loss": 1.4817,
      "step": 57300
    },
    {
      "epoch": 2.5085219823442007,
      "grad_norm": 3.143066644668579,
      "learning_rate": 4.925705794947994e-05,
      "loss": 1.5478,
      "step": 57400
    },
    {
      "epoch": 2.512892229700201,
      "grad_norm": 3.503161907196045,
      "learning_rate": 4.8820033213879906e-05,
      "loss": 1.4498,
      "step": 57500
    },
    {
      "epoch": 2.5172624770562013,
      "grad_norm": 4.9514479637146,
      "learning_rate": 4.838300847827987e-05,
      "loss": 1.4723,
      "step": 57600
    },
    {
      "epoch": 2.5216327244122017,
      "grad_norm": 4.193301200866699,
      "learning_rate": 4.794598374267983e-05,
      "loss": 1.5386,
      "step": 57700
    },
    {
      "epoch": 2.526002971768202,
      "grad_norm": 2.446453094482422,
      "learning_rate": 4.7508959007079796e-05,
      "loss": 1.4547,
      "step": 57800
    },
    {
      "epoch": 2.5303732191242023,
      "grad_norm": 1.7970495223999023,
      "learning_rate": 4.7071934271479764e-05,
      "loss": 1.5372,
      "step": 57900
    },
    {
      "epoch": 2.5347434664802027,
      "grad_norm": 2.828458547592163,
      "learning_rate": 4.6634909535879725e-05,
      "loss": 1.5121,
      "step": 58000
    },
    {
      "epoch": 2.539113713836203,
      "grad_norm": 2.1289329528808594,
      "learning_rate": 4.619788480027969e-05,
      "loss": 1.5125,
      "step": 58100
    },
    {
      "epoch": 2.5434839611922033,
      "grad_norm": 4.0025482177734375,
      "learning_rate": 4.576086006467966e-05,
      "loss": 1.5183,
      "step": 58200
    },
    {
      "epoch": 2.5478542085482037,
      "grad_norm": 6.221400737762451,
      "learning_rate": 4.532383532907962e-05,
      "loss": 1.4759,
      "step": 58300
    },
    {
      "epoch": 2.552224455904204,
      "grad_norm": 1.3008877038955688,
      "learning_rate": 4.488681059347958e-05,
      "loss": 1.5232,
      "step": 58400
    },
    {
      "epoch": 2.5565947032602043,
      "grad_norm": 3.7417240142822266,
      "learning_rate": 4.444978585787955e-05,
      "loss": 1.5261,
      "step": 58500
    },
    {
      "epoch": 2.5609649506162047,
      "grad_norm": 2.9563217163085938,
      "learning_rate": 4.401713136963552e-05,
      "loss": 1.446,
      "step": 58600
    },
    {
      "epoch": 2.565335197972205,
      "grad_norm": 4.347848892211914,
      "learning_rate": 4.3580106634035484e-05,
      "loss": 1.4959,
      "step": 58700
    },
    {
      "epoch": 2.5697054453282053,
      "grad_norm": 3.736694812774658,
      "learning_rate": 4.3143081898435445e-05,
      "loss": 1.49,
      "step": 58800
    },
    {
      "epoch": 2.5740756926842057,
      "grad_norm": 3.381472587585449,
      "learning_rate": 4.270605716283541e-05,
      "loss": 1.513,
      "step": 58900
    },
    {
      "epoch": 2.5784459400402064,
      "grad_norm": 2.739765167236328,
      "learning_rate": 4.226903242723538e-05,
      "loss": 1.4173,
      "step": 59000
    },
    {
      "epoch": 2.5828161873962068,
      "grad_norm": 4.309194564819336,
      "learning_rate": 4.183200769163534e-05,
      "loss": 1.4384,
      "step": 59100
    },
    {
      "epoch": 2.587186434752207,
      "grad_norm": 3.419282913208008,
      "learning_rate": 4.139498295603531e-05,
      "loss": 1.5296,
      "step": 59200
    },
    {
      "epoch": 2.5915566821082074,
      "grad_norm": 3.5913946628570557,
      "learning_rate": 4.095795822043527e-05,
      "loss": 1.4922,
      "step": 59300
    },
    {
      "epoch": 2.5959269294642078,
      "grad_norm": 2.2374000549316406,
      "learning_rate": 4.052093348483524e-05,
      "loss": 1.5118,
      "step": 59400
    },
    {
      "epoch": 2.600297176820208,
      "grad_norm": 2.897844076156616,
      "learning_rate": 4.00839087492352e-05,
      "loss": 1.4131,
      "step": 59500
    },
    {
      "epoch": 2.6046674241762084,
      "grad_norm": 4.11778450012207,
      "learning_rate": 3.964688401363517e-05,
      "loss": 1.4001,
      "step": 59600
    },
    {
      "epoch": 2.609037671532209,
      "grad_norm": 4.278403282165527,
      "learning_rate": 3.9209859278035136e-05,
      "loss": 1.4423,
      "step": 59700
    },
    {
      "epoch": 2.613407918888209,
      "grad_norm": 3.9143898487091064,
      "learning_rate": 3.8772834542435104e-05,
      "loss": 1.5122,
      "step": 59800
    },
    {
      "epoch": 2.6177781662442094,
      "grad_norm": 3.1735122203826904,
      "learning_rate": 3.833580980683506e-05,
      "loss": 1.5363,
      "step": 59900
    },
    {
      "epoch": 2.62214841360021,
      "grad_norm": 4.787269115447998,
      "learning_rate": 3.7898785071235026e-05,
      "loss": 1.5053,
      "step": 60000
    },
    {
      "epoch": 2.62651866095621,
      "grad_norm": 4.149543762207031,
      "learning_rate": 3.7461760335634994e-05,
      "loss": 1.4603,
      "step": 60100
    },
    {
      "epoch": 2.6308889083122105,
      "grad_norm": 3.6075589656829834,
      "learning_rate": 3.702910584739096e-05,
      "loss": 1.4651,
      "step": 60200
    },
    {
      "epoch": 2.635259155668211,
      "grad_norm": 2.497954845428467,
      "learning_rate": 3.659208111179092e-05,
      "loss": 1.4768,
      "step": 60300
    },
    {
      "epoch": 2.639629403024211,
      "grad_norm": 4.69747257232666,
      "learning_rate": 3.615505637619089e-05,
      "loss": 1.5202,
      "step": 60400
    },
    {
      "epoch": 2.6439996503802115,
      "grad_norm": 2.6805944442749023,
      "learning_rate": 3.571803164059085e-05,
      "loss": 1.473,
      "step": 60500
    },
    {
      "epoch": 2.648369897736212,
      "grad_norm": 5.0102925300598145,
      "learning_rate": 3.528100690499082e-05,
      "loss": 1.4418,
      "step": 60600
    },
    {
      "epoch": 2.652740145092212,
      "grad_norm": 2.568557024002075,
      "learning_rate": 3.4843982169390785e-05,
      "loss": 1.413,
      "step": 60700
    },
    {
      "epoch": 2.6571103924482125,
      "grad_norm": 4.901669502258301,
      "learning_rate": 3.4406957433790746e-05,
      "loss": 1.5441,
      "step": 60800
    },
    {
      "epoch": 2.661480639804213,
      "grad_norm": 3.635789632797241,
      "learning_rate": 3.3969932698190714e-05,
      "loss": 1.5389,
      "step": 60900
    },
    {
      "epoch": 2.665850887160213,
      "grad_norm": 2.360760450363159,
      "learning_rate": 3.353290796259068e-05,
      "loss": 1.478,
      "step": 61000
    },
    {
      "epoch": 2.6702211345162135,
      "grad_norm": 4.305962562561035,
      "learning_rate": 3.309588322699064e-05,
      "loss": 1.4276,
      "step": 61100
    },
    {
      "epoch": 2.6745913818722142,
      "grad_norm": 2.7800192832946777,
      "learning_rate": 3.265885849139061e-05,
      "loss": 1.5049,
      "step": 61200
    },
    {
      "epoch": 2.6789616292282146,
      "grad_norm": 7.1715474128723145,
      "learning_rate": 3.222183375579057e-05,
      "loss": 1.4992,
      "step": 61300
    },
    {
      "epoch": 2.683331876584215,
      "grad_norm": 4.0987114906311035,
      "learning_rate": 3.178480902019054e-05,
      "loss": 1.4325,
      "step": 61400
    },
    {
      "epoch": 2.6877021239402152,
      "grad_norm": 6.501972675323486,
      "learning_rate": 3.134778428459051e-05,
      "loss": 1.5099,
      "step": 61500
    },
    {
      "epoch": 2.6920723712962156,
      "grad_norm": 7.1331939697265625,
      "learning_rate": 3.091075954899047e-05,
      "loss": 1.5144,
      "step": 61600
    },
    {
      "epoch": 2.696442618652216,
      "grad_norm": 3.117668390274048,
      "learning_rate": 3.0473734813390434e-05,
      "loss": 1.4934,
      "step": 61700
    },
    {
      "epoch": 2.7008128660082162,
      "grad_norm": 3.1005499362945557,
      "learning_rate": 3.0036710077790402e-05,
      "loss": 1.5119,
      "step": 61800
    },
    {
      "epoch": 2.7051831133642166,
      "grad_norm": 2.6783149242401123,
      "learning_rate": 2.9599685342190363e-05,
      "loss": 1.5205,
      "step": 61900
    },
    {
      "epoch": 2.709553360720217,
      "grad_norm": 4.035154819488525,
      "learning_rate": 2.916266060659033e-05,
      "loss": 1.4513,
      "step": 62000
    },
    {
      "epoch": 2.7139236080762172,
      "grad_norm": 3.911921501159668,
      "learning_rate": 2.87256358709903e-05,
      "loss": 1.5166,
      "step": 62100
    },
    {
      "epoch": 2.7182938554322176,
      "grad_norm": 5.9827070236206055,
      "learning_rate": 2.828861113539026e-05,
      "loss": 1.4616,
      "step": 62200
    },
    {
      "epoch": 2.722664102788218,
      "grad_norm": 2.7733168601989746,
      "learning_rate": 2.7851586399790228e-05,
      "loss": 1.5043,
      "step": 62300
    },
    {
      "epoch": 2.7270343501442182,
      "grad_norm": 4.521846771240234,
      "learning_rate": 2.741456166419019e-05,
      "loss": 1.4941,
      "step": 62400
    },
    {
      "epoch": 2.7314045975002186,
      "grad_norm": 4.041984558105469,
      "learning_rate": 2.6977536928590157e-05,
      "loss": 1.5181,
      "step": 62500
    },
    {
      "epoch": 2.735774844856219,
      "grad_norm": 2.53779935836792,
      "learning_rate": 2.6540512192990122e-05,
      "loss": 1.5366,
      "step": 62600
    },
    {
      "epoch": 2.7401450922122192,
      "grad_norm": 1.696048617362976,
      "learning_rate": 2.6103487457390086e-05,
      "loss": 1.5077,
      "step": 62700
    },
    {
      "epoch": 2.7445153395682196,
      "grad_norm": 2.6980769634246826,
      "learning_rate": 2.566646272179005e-05,
      "loss": 1.4942,
      "step": 62800
    },
    {
      "epoch": 2.74888558692422,
      "grad_norm": 3.0163443088531494,
      "learning_rate": 2.522943798619002e-05,
      "loss": 1.5302,
      "step": 62900
    },
    {
      "epoch": 2.7532558342802202,
      "grad_norm": 3.875624418258667,
      "learning_rate": 2.479241325058998e-05,
      "loss": 1.4054,
      "step": 63000
    },
    {
      "epoch": 2.7576260816362206,
      "grad_norm": 4.429812431335449,
      "learning_rate": 2.4355388514989948e-05,
      "loss": 1.5882,
      "step": 63100
    },
    {
      "epoch": 2.761996328992221,
      "grad_norm": 2.0853002071380615,
      "learning_rate": 2.391836377938991e-05,
      "loss": 1.5324,
      "step": 63200
    },
    {
      "epoch": 2.7663665763482213,
      "grad_norm": 3.5403099060058594,
      "learning_rate": 2.3481339043789877e-05,
      "loss": 1.4655,
      "step": 63300
    },
    {
      "epoch": 2.7707368237042216,
      "grad_norm": 1.9994148015975952,
      "learning_rate": 2.304431430818984e-05,
      "loss": 1.5275,
      "step": 63400
    },
    {
      "epoch": 2.775107071060222,
      "grad_norm": 4.015676975250244,
      "learning_rate": 2.2607289572589806e-05,
      "loss": 1.4282,
      "step": 63500
    },
    {
      "epoch": 2.7794773184162223,
      "grad_norm": 4.490049362182617,
      "learning_rate": 2.217026483698977e-05,
      "loss": 1.4946,
      "step": 63600
    },
    {
      "epoch": 2.7838475657722226,
      "grad_norm": 3.920884132385254,
      "learning_rate": 2.173324010138974e-05,
      "loss": 1.4752,
      "step": 63700
    },
    {
      "epoch": 2.788217813128223,
      "grad_norm": 8.18712329864502,
      "learning_rate": 2.12962153657897e-05,
      "loss": 1.4301,
      "step": 63800
    },
    {
      "epoch": 2.7925880604842233,
      "grad_norm": 3.687326431274414,
      "learning_rate": 2.0859190630189668e-05,
      "loss": 1.4585,
      "step": 63900
    },
    {
      "epoch": 2.7969583078402236,
      "grad_norm": 5.480653762817383,
      "learning_rate": 2.042216589458963e-05,
      "loss": 1.567,
      "step": 64000
    },
    {
      "epoch": 2.801328555196224,
      "grad_norm": 5.1585493087768555,
      "learning_rate": 1.9985141158989597e-05,
      "loss": 1.42,
      "step": 64100
    },
    {
      "epoch": 2.8056988025522243,
      "grad_norm": 2.801410675048828,
      "learning_rate": 1.9548116423389565e-05,
      "loss": 1.4466,
      "step": 64200
    },
    {
      "epoch": 2.8100690499082246,
      "grad_norm": 2.770124912261963,
      "learning_rate": 1.9111091687789526e-05,
      "loss": 1.468,
      "step": 64300
    },
    {
      "epoch": 2.814439297264225,
      "grad_norm": 1.79755699634552,
      "learning_rate": 1.8674066952189494e-05,
      "loss": 1.3793,
      "step": 64400
    },
    {
      "epoch": 2.8188095446202253,
      "grad_norm": 1.4219489097595215,
      "learning_rate": 1.823704221658946e-05,
      "loss": 1.4252,
      "step": 64500
    },
    {
      "epoch": 2.8231797919762256,
      "grad_norm": 2.3193931579589844,
      "learning_rate": 1.7800017480989423e-05,
      "loss": 1.5133,
      "step": 64600
    },
    {
      "epoch": 2.827550039332226,
      "grad_norm": 3.3418166637420654,
      "learning_rate": 1.7362992745389388e-05,
      "loss": 1.4334,
      "step": 64700
    },
    {
      "epoch": 2.8319202866882267,
      "grad_norm": 2.782520055770874,
      "learning_rate": 1.6925968009789352e-05,
      "loss": 1.4309,
      "step": 64800
    },
    {
      "epoch": 2.836290534044227,
      "grad_norm": 2.527557373046875,
      "learning_rate": 1.6488943274189317e-05,
      "loss": 1.468,
      "step": 64900
    },
    {
      "epoch": 2.8406607814002274,
      "grad_norm": 2.6500163078308105,
      "learning_rate": 1.605191853858928e-05,
      "loss": 1.4116,
      "step": 65000
    },
    {
      "epoch": 2.8450310287562277,
      "grad_norm": 3.1675586700439453,
      "learning_rate": 1.561489380298925e-05,
      "loss": 1.526,
      "step": 65100
    },
    {
      "epoch": 2.849401276112228,
      "grad_norm": 6.568499565124512,
      "learning_rate": 1.5177869067389214e-05,
      "loss": 1.4755,
      "step": 65200
    },
    {
      "epoch": 2.8537715234682284,
      "grad_norm": 4.995077133178711,
      "learning_rate": 1.4740844331789178e-05,
      "loss": 1.4696,
      "step": 65300
    },
    {
      "epoch": 2.8581417708242287,
      "grad_norm": 3.3114523887634277,
      "learning_rate": 1.4303819596189143e-05,
      "loss": 1.5107,
      "step": 65400
    },
    {
      "epoch": 2.862512018180229,
      "grad_norm": 2.776475191116333,
      "learning_rate": 1.386679486058911e-05,
      "loss": 1.5234,
      "step": 65500
    },
    {
      "epoch": 2.8668822655362294,
      "grad_norm": 3.6772537231445312,
      "learning_rate": 1.3429770124989074e-05,
      "loss": 1.4674,
      "step": 65600
    },
    {
      "epoch": 2.8712525128922297,
      "grad_norm": 2.106064796447754,
      "learning_rate": 1.2992745389389038e-05,
      "loss": 1.4327,
      "step": 65700
    },
    {
      "epoch": 2.87562276024823,
      "grad_norm": 2.5747413635253906,
      "learning_rate": 1.2555720653789003e-05,
      "loss": 1.408,
      "step": 65800
    },
    {
      "epoch": 2.8799930076042304,
      "grad_norm": 5.672929286956787,
      "learning_rate": 1.211869591818897e-05,
      "loss": 1.4878,
      "step": 65900
    },
    {
      "epoch": 2.8843632549602307,
      "grad_norm": 4.198362827301025,
      "learning_rate": 1.1681671182588934e-05,
      "loss": 1.4762,
      "step": 66000
    },
    {
      "epoch": 2.888733502316231,
      "grad_norm": 2.850156545639038,
      "learning_rate": 1.1244646446988898e-05,
      "loss": 1.4577,
      "step": 66100
    },
    {
      "epoch": 2.8931037496722314,
      "grad_norm": Infinity,
      "learning_rate": 1.0807621711388863e-05,
      "loss": 1.4832,
      "step": 66200
    },
    {
      "epoch": 2.8974739970282317,
      "grad_norm": 3.782590627670288,
      "learning_rate": 1.0374967223144829e-05,
      "loss": 1.4392,
      "step": 66300
    },
    {
      "epoch": 2.901844244384232,
      "grad_norm": 3.632230758666992,
      "learning_rate": 9.937942487544794e-06,
      "loss": 1.4686,
      "step": 66400
    },
    {
      "epoch": 2.9062144917402324,
      "grad_norm": 7.119418144226074,
      "learning_rate": 9.500917751944758e-06,
      "loss": 1.4565,
      "step": 66500
    },
    {
      "epoch": 2.9105847390962327,
      "grad_norm": 3.4954965114593506,
      "learning_rate": 9.063893016344724e-06,
      "loss": 1.4233,
      "step": 66600
    },
    {
      "epoch": 2.914954986452233,
      "grad_norm": 3.5922396183013916,
      "learning_rate": 8.626868280744689e-06,
      "loss": 1.4469,
      "step": 66700
    },
    {
      "epoch": 2.9193252338082334,
      "grad_norm": 3.142146348953247,
      "learning_rate": 8.189843545144655e-06,
      "loss": 1.4744,
      "step": 66800
    },
    {
      "epoch": 2.923695481164234,
      "grad_norm": 2.811793804168701,
      "learning_rate": 7.75281880954462e-06,
      "loss": 1.4516,
      "step": 66900
    },
    {
      "epoch": 2.9280657285202345,
      "grad_norm": 2.152036428451538,
      "learning_rate": 7.315794073944584e-06,
      "loss": 1.4089,
      "step": 67000
    },
    {
      "epoch": 2.932435975876235,
      "grad_norm": 3.375817060470581,
      "learning_rate": 6.878769338344549e-06,
      "loss": 1.5052,
      "step": 67100
    },
    {
      "epoch": 2.936806223232235,
      "grad_norm": 2.592674970626831,
      "learning_rate": 6.441744602744515e-06,
      "loss": 1.4837,
      "step": 67200
    },
    {
      "epoch": 2.9411764705882355,
      "grad_norm": 5.606136798858643,
      "learning_rate": 6.00471986714448e-06,
      "loss": 1.4726,
      "step": 67300
    },
    {
      "epoch": 2.945546717944236,
      "grad_norm": 3.9933362007141113,
      "learning_rate": 5.567695131544445e-06,
      "loss": 1.4531,
      "step": 67400
    },
    {
      "epoch": 2.949916965300236,
      "grad_norm": 3.2270703315734863,
      "learning_rate": 5.1306703959444105e-06,
      "loss": 1.3998,
      "step": 67500
    },
    {
      "epoch": 2.9542872126562365,
      "grad_norm": 3.1721043586730957,
      "learning_rate": 4.693645660344375e-06,
      "loss": 1.4921,
      "step": 67600
    },
    {
      "epoch": 2.958657460012237,
      "grad_norm": 1.9363099336624146,
      "learning_rate": 4.2566209247443405e-06,
      "loss": 1.5157,
      "step": 67700
    },
    {
      "epoch": 2.963027707368237,
      "grad_norm": 3.0063979625701904,
      "learning_rate": 3.819596189144305e-06,
      "loss": 1.4271,
      "step": 67800
    },
    {
      "epoch": 2.9673979547242375,
      "grad_norm": 4.670900821685791,
      "learning_rate": 3.3825714535442705e-06,
      "loss": 1.5378,
      "step": 67900
    },
    {
      "epoch": 2.971768202080238,
      "grad_norm": 4.110760688781738,
      "learning_rate": 2.9455467179442354e-06,
      "loss": 1.4626,
      "step": 68000
    },
    {
      "epoch": 2.976138449436238,
      "grad_norm": 5.765872955322266,
      "learning_rate": 2.5085219823442004e-06,
      "loss": 1.4571,
      "step": 68100
    },
    {
      "epoch": 2.9805086967922385,
      "grad_norm": 5.804752826690674,
      "learning_rate": 2.0714972467441654e-06,
      "loss": 1.4581,
      "step": 68200
    },
    {
      "epoch": 2.984878944148239,
      "grad_norm": 3.828470468521118,
      "learning_rate": 1.6344725111441306e-06,
      "loss": 1.4908,
      "step": 68300
    },
    {
      "epoch": 2.989249191504239,
      "grad_norm": 3.0720529556274414,
      "learning_rate": 1.1974477755440956e-06,
      "loss": 1.5406,
      "step": 68400
    },
    {
      "epoch": 2.9936194388602395,
      "grad_norm": 3.0354208946228027,
      "learning_rate": 7.647932873000612e-07,
      "loss": 1.4453,
      "step": 68500
    }
  ],
  "logging_steps": 100,
  "max_steps": 68646,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 2.425565208379392e+18,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
