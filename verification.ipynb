{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3915df28",
   "metadata": {},
   "source": [
    "# Génération de code et tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9e2f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nelld\\Desktop\\N7\\2SNIA\\llamaenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\nelld\\Desktop\\N7\\2SNIA\\llamaenv\\Lib\\site-packages\\transformers\\modeling_utils.py:5803: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:28.)\n",
      "  _ = torch.empty(byte_count // factor, dtype=torch.float16, device=device, requires_grad=False)\n",
      "Device set to use cuda:0\n",
      "Generating completions:   0%|          | 0/20 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "c:\\Users\\nelld\\Desktop\\N7\\2SNIA\\llamaenv\\Lib\\site-packages\\bitsandbytes\\nn\\modules.py:451: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(\n",
      "Generating completions:  50%|█████     | 10/20 [00:47<00:44,  4.46s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "Generating completions: 100%|██████████| 20/20 [01:36<00:00,  4.82s/it]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from parser import *\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\n",
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "### PARAMETERS\n",
    "\n",
    "filename = \"./test.json\"\n",
    "gen_per_prompt = 3\n",
    "summary_output = \"./stats.md\"\n",
    "generation_output = \"./gens.txt\"\n",
    "\n",
    "# Path to the extracted model\n",
    "model_path = \"./starcoder_elm_finetuned_2\"\n",
    "\n",
    "# Load the model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "bnb_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, quantization_config=bnb_config)\n",
    "\n",
    "# Setup the generation pipeline\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Load the JSON data\n",
    "with open(filename) as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "successes = 0\n",
    "fails = 0\n",
    "\n",
    "with open(summary_output, 'w') as f2:\n",
    "    with open(generation_output, 'w') as f3:\n",
    "        # Header\n",
    "        f2.write(\"| Prompt No | Successes | Fails | Success rate |\\n\")\n",
    "        f2.write(\"| ----------- | ----------- | ----------- | ----------- |\\n\")\n",
    "\n",
    "        # Progress bar here\n",
    "        for num, item in enumerate(tqdm(data, desc=\"Generating completions\")):\n",
    "            prompt = item[\"prompt\"]\n",
    "\n",
    "            prompt_successes = 0\n",
    "            prompt_fails = 0\n",
    "\n",
    "            results = generator(prompt, max_length=50, num_return_sequences=gen_per_prompt, do_sample=True, temperature=0.7, top_p=0.95)\n",
    "            for i, result in enumerate(results):\n",
    "                try:\n",
    "                    parsed, msg = parse_elm(result[\"generated_text\"])\n",
    "                    if parsed:\n",
    "                        prompt_successes += 1\n",
    "                    else:\n",
    "                        prompt_fails += 1\n",
    "                except:\n",
    "                    prompt_fails += 1\n",
    "\n",
    "                f3.write(f\"--- Génération {i+1} ---\\n\")\n",
    "                f3.write(result['generated_text'])\n",
    "                f3.write(\"\\n\\n\")\n",
    "\n",
    "            line = f\"| {num} | {prompt_successes} | {prompt_fails} | {100 * prompt_successes / (prompt_successes + prompt_fails)} |\\n\"\n",
    "            f2.write(line)\n",
    "\n",
    "            if parsed:\n",
    "                successes += prompt_successes\n",
    "            else:\n",
    "                fails += prompt_fails\n",
    "\n",
    "        f2.write(\"\\n\")\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    f2.write(f\"Number of tries: {successes + fails}\\n\")\n",
    "    f2.write(f\"Number of successes: {successes}\\n\")\n",
    "    f2.write(f\"Number of fails: {fails}\\n\")\n",
    "    f2.write(f\"Succes rate: {100 * successes / (successes + fails)}%\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamaenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
